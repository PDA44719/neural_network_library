{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cecd8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7183f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size, gain = 1.0):\n",
    "    \"\"\"\n",
    "    Xavier initialization of network weights.\n",
    "\n",
    "    Arguments:\n",
    "        - size {tuple} -- size of the network to initialise.\n",
    "        - gain {float} -- gain for the Xavier initialisation.\n",
    "\n",
    "    Returns:\n",
    "        {np.ndarray} -- values of the weights.\n",
    "    \"\"\"\n",
    "    low = -gain * np.sqrt(6.0 / np.sum(size))\n",
    "    high = gain * np.sqrt(6.0 / np.sum(size))\n",
    "    return np.random.uniform(low=low, high=high, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b73a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Abstract layer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_params(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37cbd21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELossLayer(Layer):\n",
    "    \"\"\"\n",
    "    MSELossLayer: Computes mean-squared error between y_pred and y_target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse(y_pred, y_target):\n",
    "        return np.mean((y_pred - y_target) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse_grad(y_pred, y_target):\n",
    "        return 2 * (y_pred - y_target) / len(y_pred)\n",
    "\n",
    "    def forward(self, y_pred, y_target):\n",
    "        self._cache_current = y_pred, y_target\n",
    "        return self._mse(y_pred, y_target)\n",
    "\n",
    "    def backward(self):\n",
    "        return self._mse_grad(*self._cache_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3c2fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossLayer(Layer):\n",
    "    \"\"\"\n",
    "    CrossEntropyLossLayer: Computes the softmax followed by the negative \n",
    "    log-likelihood loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        numer = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        denom = numer.sum(axis=1, keepdims=True)\n",
    "        return numer / denom\n",
    "\n",
    "    def forward(self, inputs, y_target):\n",
    "        assert len(inputs) == len(y_target)\n",
    "        n_obs = len(y_target)\n",
    "        probs = self.softmax(inputs)\n",
    "        self._cache_current = y_target, probs\n",
    "\n",
    "        out = -1 / n_obs * np.sum(y_target * np.log(probs))\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        y_target, probs = self._cache_current\n",
    "        n_obs = len(y_target)\n",
    "        return -1 / n_obs * (y_target - probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4018f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer(Layer):\n",
    "    \"\"\"\n",
    "    SigmoidLayer: Applies sigmoid function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Constructor of the Sigmoid layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Sigmoid layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        output = 1/(1 + np.exp(-x))\n",
    "        self._cache_current = output\n",
    "        return output\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        output = self._cache_current\n",
    "        return grad_z * (1 - output) * output\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d1f6b",
   "metadata": {},
   "source": [
    "I believe this is how it works. Because, we are given $\\frac{\\partial Loss}{\\partial Z}$, and we want to obtain $\\frac{\\partial Loss}{\\partial X} = \\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial X}$. This is equal to multiplying the **grad_z** ($\\frac{\\partial Loss}{\\partial Z}$) by the derivative of the sigmoid function ($\\frac{\\partial Z}{\\partial X}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cabf663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88079708]\n",
      "[0.10499359]\n"
     ]
    }
   ],
   "source": [
    "s = SigmoidLayer()\n",
    "a = np.array([2])\n",
    "print(s.forward(a))\n",
    "print(s.backward(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccf58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d179423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1beb90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1050])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pda12_unix/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "t = torch.Tensor([2])\n",
    "l = torch.nn.Parameter(t)\n",
    "sigmoid(l).backward()\n",
    "print(l.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37e8402c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3303402067.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_877/3303402067.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sigmoid.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3535f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.transpose(t, 0, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "568eccb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_877/2388712034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'value_loss' is not defined"
     ]
    }
   ],
   "source": [
    "a * value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a75d8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluLayer(Layer):\n",
    "    \"\"\"\n",
    "    ReluLayer: Applies Relu function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of the Relu layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Relu layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        output =  x\n",
    "        output[output<=0] = 0\n",
    "        self._cache_current = output\n",
    "        return output\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        relu_derivative = self._cache_current\n",
    "        relu_derivative[relu_derivative > 0] = 1\n",
    "        return grad_z * relu_derivative\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92bb8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 5 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, -2, 5, 0])\n",
    "rl = ReluLayer()\n",
    "print(rl.forward(a))\n",
    "rl.backward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7aa6ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0968,  0.0787,  0.3657, -0.2421,  0.2171],\n",
      "        [ 0.3219,  0.1358, -0.0431, -0.2225, -0.3177],\n",
      "        [-0.0509, -0.4332,  0.2034, -0.0082, -0.1672]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1197,  0.4997, -0.3725], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "l = torch.nn.Linear(5, 3)\n",
    "a = torch.Tensor([1, 0, 0, 0, 0])\n",
    "print(l.weight)\n",
    "l(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c91d89ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([[2], [1]])\n",
    "np.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09d102c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cecaadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([[1, 2], [3, 4]])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c09fe98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7192fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5abc2faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43880357, 0.95491541, 0.53560563, 0.1189112 , 0.66806565],\n",
       "       [0.43880357, 0.95491541, 0.53560563, 0.1189112 , 0.66806565],\n",
       "       [0.43880357, 0.95491541, 0.53560563, 0.1189112 , 0.66806565],\n",
       "       [0.43880357, 0.95491541, 0.53560563, 0.1189112 , 0.66806565],\n",
       "       [0.43880357, 0.95491541, 0.53560563, 0.1189112 , 0.66806565]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_biases = np.random.rand(1, 5)\n",
    "np.concatenate([initial_biases] * 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64374c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.ones(shape=(1, 5)).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34c2daad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.LinearLayer'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = LinearLayer(10, 1)\n",
    "print(type(a))\n",
    "print(isinstance(a, SigmoidLayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c27cb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  7,  8],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([5, 5, 5])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7fbb67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \"\"\"\n",
    "    LinearLayer: Performs affine transformation of input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Constructor of the linear layer.\n",
    "\n",
    "        Arguments:\n",
    "            - n_in {int} -- Number (or dimension) of inputs.\n",
    "            - n_out {int} -- Number (or dimension) of outputs.\n",
    "        \"\"\"\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._W = None\n",
    "        self._b = None\n",
    "\n",
    "        self._cache_current = None\n",
    "        self._grad_W_current = None\n",
    "        self._grad_b_current = None\n",
    "        \n",
    "        self._W = np.random.rand(n_in, n_out)\n",
    "        self._b = np.random.rand(1, n_out)\n",
    "        print(self._b)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the layer (i.e. returns Wx + b).\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._cache_current = x\n",
    "        #B_matrix = np.concatenate([self._b] * len(x), 0)\n",
    "        #print(B_matrix.shape)\n",
    "        return np.matmul(x, self._W)  + self._b\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._grad_W_current = np.matmul(self._cache_current.transpose(), grad_z)\n",
    "        self._grad_b_current = np.matmul(np.ones(shape=(1, grad_z.shape[0])), grad_z)\n",
    "        return np.matmul(grad_z, self._W.transpose())\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        layer's parameters using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._b -= learning_rate * self._grad_b_current\n",
    "        self._W -= learning_rate * self._grad_W_current\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da562a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[2, 2, 2], [3, 3, 3]])\n",
    "np.ones_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8962284d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "c * np.ones_like(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34710ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer(Layer):\n",
    "    \"\"\"\n",
    "    IdentityLayer: Applies Identity function elementwise.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of the linear layer.\n",
    "\n",
    "        Arguments:\n",
    "            - n_in {int} -- Number (or dimension) of inputs.\n",
    "            - n_out {int} -- Number (or dimension) of outputs.\n",
    "        \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Relu layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return x\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return grad_z\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fcb2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_or_loss_layer(layer_name: str):\n",
    "    if layer_name == 'relu':\n",
    "        return ReluLayer()\n",
    "    elif layer_name == 'sigmoid':\n",
    "        return SigmoidLayer()\n",
    "    elif layer_name == 'identity':\n",
    "        return IdentityLayer()\n",
    "    elif layer_name == 'cross_entropy':\n",
    "        return CrossEntropyLossLayer()\n",
    "    elif layer_name == 'mse':\n",
    "        return MSELossLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85658a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "1\n",
      "[1, 10, 5]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 10, 5]\n",
    "for i in reversed(a):\n",
    "    print(i)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8f88e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNetwork(object):\n",
    "    \"\"\"\n",
    "    MultiLayerNetwork: A network consisting of stacked linear layers and\n",
    "    activation functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, neurons, activations):\n",
    "        \"\"\"\n",
    "        Constructor of the multi layer network.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dim {int} -- Number of features in the input (excluding \n",
    "                the batch dimension).\n",
    "            - neurons {list} -- Number of neurons in each linear layer \n",
    "                represented as a list. The length of the list determines the \n",
    "                number of linear layers.\n",
    "            - activations {list} -- List of the activation functions to apply \n",
    "                to the output of each linear layer.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.neurons = neurons\n",
    "        self.activations = activations\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._layers = []\n",
    "        \n",
    "        # Append all the layers and activation functions\n",
    "        current_in_dim = self.input_dim\n",
    "        for i in range (0, len(neurons)):\n",
    "            self._layers.append(LinearLayer(current_in_dim, neurons[i]))\n",
    "            self._layers.append(get_act_or_loss_layer(activations[i]))\n",
    "            current_in_dim = neurons[i]\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size,\n",
    "                #_neurons_in_final_layer)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        for layer in self._layers:\n",
    "            x = layer.forward(x)\n",
    "        return x # Replace with your own code\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Performs backward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size,\n",
    "                #_neurons_in_final_layer).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        for layer in reversed(self._layers):\n",
    "            grad_z = layer.backward(grad_z)\n",
    "        \n",
    "        return grad_z\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        parameters of all layers using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        for layer in self._layers:\n",
    "            if isinstance(layer, LinearLayer):\n",
    "                layer.update_params(learning_rate)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b229aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network(network, fpath):\n",
    "    \"\"\"\n",
    "    Utility function to pickle `network` at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        pickle.dump(network, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce11bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(fpath):\n",
    "    \"\"\"\n",
    "    Utility function to load network found at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        network = pickle.load(f)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "896f0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ecb6f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = random.sample(range(3), k=3)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aab3b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4483cd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "a[[2, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ae19f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(input_dataset, target_dataset, batch_size):\n",
    "    index_start = 0\n",
    "    index_end = batch_size\n",
    "    input_batches, target_batches = [], []\n",
    "    while index_end <= len(input_dataset):\n",
    "        input_batches.append(input_dataset[index_start:index_end])\n",
    "        target_batches.append(target_dataset[index_start:index_end])\n",
    "        index_start += batch_size\n",
    "        index_end += batch_size\n",
    "    return input_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7c5f8e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[array([1, 2]), array([3, 4]), array([5, 6]), array([7, 8])]\n",
      "[array([1, 2]), array([3, 4]), array([5, 6]), array([7, 8])]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "b = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(len(b))\n",
    "a_batches, b_batches = split_into_batches(a, b, 2)\n",
    "print(a_batches)\n",
    "print(b_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e644d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trainer: Object that manages the training of a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        batch_size,\n",
    "        nb_epoch,\n",
    "        learning_rate,\n",
    "        loss_fun,\n",
    "        shuffle_flag,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor of the Trainer.\n",
    "\n",
    "        Arguments:\n",
    "            - network {MultiLayerNetwork} -- MultiLayerNetwork to be trained.\n",
    "            - batch_size {int} -- Training batch size.\n",
    "            - nb_epoch {int} -- Number of training epochs.\n",
    "            - learning_rate {float} -- SGD learning rate to be used in training.\n",
    "            - loss_fun {str} -- Loss function to be used. Possible values: mse,\n",
    "                cross_entropy.\n",
    "            - shuffle_flag {bool} -- If True, training data is shuffled before\n",
    "                training.\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fun = loss_fun\n",
    "        self.shuffle_flag = shuffle_flag\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._loss_layer = get_act_or_loss_layer(self.loss_fun)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Returns shuffled versions of the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_data_points, n_features) or (#_data_points,).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_data_points, #output_neurons).\n",
    "\n",
    "        Returns: \n",
    "            - {np.ndarray} -- shuffled inputs.\n",
    "            - {np.ndarray} -- shuffled_targets.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        # Shuffle indices\n",
    "        shuffled_indices = random.sample(range(len(input_dataset)), k=len(input_dataset))\n",
    "        \n",
    "        # Get the dataset in the order of the shuffled indices\n",
    "        shuffled_inputs = input_dataset[shuffled_indices]\n",
    "        shuffled_targets = target_dataset[shuffled_indices]\n",
    "        \n",
    "        return shuffled_inputs, shuffled_targets \n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def train(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Main training loop. Performs the following steps `nb_epoch` times:\n",
    "            - Shuffles the input data (if `shuffle` is True)\n",
    "            - Splits the dataset into batches of size `batch_size`.\n",
    "            - For each batch:\n",
    "                - Performs forward pass through the network given the current\n",
    "                batch of inputs.\n",
    "                - Computes loss.\n",
    "                - Performs backward pass to compute gradients of loss with\n",
    "                respect to parameters of network.\n",
    "                - Performs one step of gradient descent on the network\n",
    "                parameters.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_training_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_training_data_points, #output_neurons).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        # Shuffle dataset if required\n",
    "        if self.shuffle_flag:\n",
    "            input_dataset, target_dataset = self.shuffle(input_dataset, target_dataset)\n",
    "        \n",
    "        # Split dataset into batches\n",
    "        input_batches, target_batches = split_into_batches(input_dataset, target_dataset, self.batch_size)\n",
    "        \n",
    "        # Repeat the process for the chosen number of epochs\n",
    "        for i in range(self.nb_epoch):\n",
    "            running_loss = 0\n",
    "            # Obtain the output, compute the loss and perform gradient descent for each batch\n",
    "            for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "                output = self.network(input_batch)\n",
    "                loss = self._loss_layer(output, target_batch)\n",
    "                self.network.backward(self._loss_layer.backward())\n",
    "                self.network.update_params(self.learning_rate)\n",
    "                running_loss += loss\n",
    "            \n",
    "            print(f'Epoch {i+1} => Avg Loss = {running_loss/len(input_batches)}')\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def eval_loss(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Function that evaluate the loss function for given data. Returns\n",
    "        scalar value.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_evaluation_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_evaluation_data_points, #output_neurons).\n",
    "        \n",
    "        Returns:\n",
    "            a scalar value -- the loss\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        output = self.network(input_dataset)\n",
    "        loss = self._loss_layer(output, target_dataset)\n",
    "        return loss\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dbbd809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [7 8 9]\n",
      " [4 5 6]]\n",
      "[['A']\n",
      " ['C']\n",
      " ['B']]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "b = np.array([['A'], ['B'], ['C']])\n",
    "a_s, b_s = Trainer.shuffle(a, b)\n",
    "print(a_s)\n",
    "print(b_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "51446815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    Preprocessor: Object used to apply \"preprocessing\" operation to datasets.\n",
    "    The object can also be used to revert the changes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the Preprocessor according to the provided dataset.\n",
    "        (Does not modify the dataset.)\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset used to determine the parameters for\n",
    "            the normalization.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._max_value = data.max()\n",
    "        self._min_value = data.min()\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def apply(self, data):\n",
    "        \"\"\"\n",
    "        Apply the pre-processing operations to the provided dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} normalized dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return (data - self._min_value) / (self._max_value - self._min_value)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def revert(self, data):\n",
    "        \"\"\"\n",
    "        Revert the pre-processing operations to retrieve the original dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset for which to revert normalization.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} reverted dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return data * (self._max_value - self._min_value) + self._min_value\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8a6e3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.2]\n",
      " [1.  0.8]]\n",
      "[[-2.  0.]\n",
      " [ 8.  6.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[-2,  0], [8, 6]])\n",
    "prep = Preprocessor(a)\n",
    "a_preprocessed = prep.apply(a)\n",
    "print(a_preprocessed)\n",
    "a_reverted = prep.revert(a_preprocessed)\n",
    "print(a_reverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "35efc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_main():\n",
    "    input_dim = 4\n",
    "    neurons = [16, 3]\n",
    "    activations = [\"relu\", \"identity\"]\n",
    "    net = MultiLayerNetwork(input_dim, neurons, activations)\n",
    "\n",
    "    dat = np.loadtxt(\"iris.dat\")\n",
    "    np.random.shuffle(dat)\n",
    "\n",
    "    x = dat[:, :4]\n",
    "    y = dat[:, 4:]\n",
    "\n",
    "    split_idx = int(0.8 * len(x))\n",
    "\n",
    "    x_train = x[:split_idx]\n",
    "    y_train = y[:split_idx]\n",
    "    x_val = x[split_idx:]\n",
    "    y_val = y[split_idx:]\n",
    "\n",
    "    prep_input = Preprocessor(x_train)\n",
    "\n",
    "    x_train_pre = prep_input.apply(x_train)\n",
    "    x_val_pre = prep_input.apply(x_val)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        network=net,\n",
    "        batch_size=8,\n",
    "        nb_epoch=1000,\n",
    "        learning_rate=0.01,\n",
    "        loss_fun=\"cross_entropy\",\n",
    "        shuffle_flag=True,\n",
    "    )\n",
    "\n",
    "    trainer.train(x_train_pre, y_train)\n",
    "    print(f\"Train loss = {trainer.eval_loss(x_train_pre, y_train)}\")\n",
    "    print(f\"Validation loss = {trainer.eval_loss(x_val_pre, y_val)}\")\n",
    "\n",
    "    preds = net(x_val_pre).argmax(axis=1).squeeze()\n",
    "    targets = y_val.argmax(axis=1).squeeze()\n",
    "    accuracy = (preds == targets).mean()\n",
    "    print(f\"Validation accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dee79b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01410092 0.8506121  0.38523912 0.01377887 0.20025755 0.53115905\n",
      "  0.6210272  0.80000277 0.76799672 0.00503631 0.37628902 0.07971439\n",
      "  0.49429395 0.32952723 0.40066377 0.88464664]]\n",
      "[[0.05765671 0.63794632 0.68076062]]\n",
      "Epoch 1 => Avg Loss = 1.185310100220246\n",
      "Epoch 2 => Avg Loss = 1.1646673459624408\n",
      "Epoch 3 => Avg Loss = 1.1555867204139214\n",
      "Epoch 4 => Avg Loss = 1.1469654529511792\n",
      "Epoch 5 => Avg Loss = 1.1385010658257713\n",
      "Epoch 6 => Avg Loss = 1.1301631466886586\n",
      "Epoch 7 => Avg Loss = 1.1219363809781728\n",
      "Epoch 8 => Avg Loss = 1.1138079213685037\n",
      "Epoch 9 => Avg Loss = 1.1057660348814797\n",
      "Epoch 10 => Avg Loss = 1.097799886505313\n",
      "Epoch 11 => Avg Loss = 1.0898994729343618\n",
      "Epoch 12 => Avg Loss = 1.0820555802493106\n",
      "Epoch 13 => Avg Loss = 1.0742597481942338\n",
      "Epoch 14 => Avg Loss = 1.0665042382916716\n",
      "Epoch 15 => Avg Loss = 1.058782005038373\n",
      "Epoch 16 => Avg Loss = 1.0510866697202303\n",
      "Epoch 17 => Avg Loss = 1.043412496446596\n",
      "Epoch 18 => Avg Loss = 1.0357543700322274\n",
      "Epoch 19 => Avg Loss = 1.0281077753776726\n",
      "Epoch 20 => Avg Loss = 1.020468778020038\n",
      "Epoch 21 => Avg Loss = 1.012834005546863\n",
      "Epoch 22 => Avg Loss = 1.0052006295866458\n",
      "Epoch 23 => Avg Loss = 0.9975663481108276\n",
      "Epoch 24 => Avg Loss = 0.9899293678040038\n",
      "Epoch 25 => Avg Loss = 0.9822883862821234\n",
      "Epoch 26 => Avg Loss = 0.9746425739624952\n",
      "Epoch 27 => Avg Loss = 0.9669915554147851\n",
      "Epoch 28 => Avg Loss = 0.9593353900487932\n",
      "Epoch 29 => Avg Loss = 0.9516745520225932\n",
      "Epoch 30 => Avg Loss = 0.944009909283496\n",
      "Epoch 31 => Avg Loss = 0.936342701683957\n",
      "Epoch 32 => Avg Loss = 0.9286745181447624\n",
      "Epoch 33 => Avg Loss = 0.9210072728681095\n",
      "Epoch 34 => Avg Loss = 0.9133431806332055\n",
      "Epoch 35 => Avg Loss = 0.9056847312361427\n",
      "Epoch 36 => Avg Loss = 0.8980346631636176\n",
      "Epoch 37 => Avg Loss = 0.8903959366159631\n",
      "Epoch 38 => Avg Loss = 0.8827717060184043\n",
      "Epoch 39 => Avg Loss = 0.8751652921799841\n",
      "Epoch 40 => Avg Loss = 0.8675801542766947\n",
      "Epoch 41 => Avg Loss = 0.8600198618486771\n",
      "Epoch 42 => Avg Loss = 0.8524880670105657\n",
      "Epoch 43 => Avg Loss = 0.8449884770790032\n",
      "Epoch 44 => Avg Loss = 0.8375248278218973\n",
      "Epoch 45 => Avg Loss = 0.8301008575302357\n",
      "Epoch 46 => Avg Loss = 0.8227202821053093\n",
      "Epoch 47 => Avg Loss = 0.815386771342337\n",
      "Epoch 48 => Avg Loss = 0.8081039265760893\n",
      "Epoch 49 => Avg Loss = 0.8008752598356447\n",
      "Epoch 50 => Avg Loss = 0.7937041746344132\n",
      "Epoch 51 => Avg Loss = 0.7865939484986594\n",
      "Epoch 52 => Avg Loss = 0.7795477173135047\n",
      "Epoch 53 => Avg Loss = 0.7725684615405038\n",
      "Epoch 54 => Avg Loss = 0.7656589943359169\n",
      "Epoch 55 => Avg Loss = 0.7588219515743687\n",
      "Epoch 56 => Avg Loss = 0.7520597837592383\n",
      "Epoch 57 => Avg Loss = 0.745374749779252\n",
      "Epoch 58 => Avg Loss = 0.7387689124508783\n",
      "Epoch 59 => Avg Loss = 0.7322441357684235\n",
      "Epoch 60 => Avg Loss = 0.7258020837685016\n",
      "Epoch 61 => Avg Loss = 0.7194442209029339\n",
      "Epoch 62 => Avg Loss = 0.7131718138041282\n",
      "Epoch 63 => Avg Loss = 0.7069859343196436\n",
      "Epoch 64 => Avg Loss = 0.7008874636878216\n",
      "Epoch 65 => Avg Loss = 0.6948770977239406\n",
      "Epoch 66 => Avg Loss = 0.6889553528861541\n",
      "Epoch 67 => Avg Loss = 0.6831225730922381\n",
      "Epoch 68 => Avg Loss = 0.6773789371617108\n",
      "Epoch 69 => Avg Loss = 0.6717244667628943\n",
      "Epoch 70 => Avg Loss = 0.666159034750721\n",
      "Epoch 71 => Avg Loss = 0.6606823737882951\n",
      "Epoch 72 => Avg Loss = 0.6552940851531422\n",
      "Epoch 73 => Avg Loss = 0.6499936476374976\n",
      "Epoch 74 => Avg Loss = 0.6447804264606689\n",
      "Epoch 75 => Avg Loss = 0.6396536821203125\n",
      "Epoch 76 => Avg Loss = 0.6346125791181663\n",
      "Epoch 77 => Avg Loss = 0.629656194504303\n",
      "Epoch 78 => Avg Loss = 0.6247835261921519\n",
      "Epoch 79 => Avg Loss = 0.6199935010043107\n",
      "Epoch 80 => Avg Loss = 0.615284982416476\n",
      "Epoch 81 => Avg Loss = 0.6106567779735713\n",
      "Epoch 82 => Avg Loss = 0.6061076463583761\n",
      "Epoch 83 => Avg Loss = 0.6016363040985747\n",
      "Epoch 84 => Avg Loss = 0.5972414319032084\n",
      "Epoch 85 => Avg Loss = 0.5929216806239974\n",
      "Epoch 86 => Avg Loss = 0.5886756768409267\n",
      "Epoch 87 => Avg Loss = 0.5845020280749089\n",
      "Epoch 88 => Avg Loss = 0.5803993276332486\n",
      "Epoch 89 => Avg Loss = 0.5763661590960704\n",
      "Epoch 90 => Avg Loss = 0.5724011004539193\n",
      "Epoch 91 => Avg Loss = 0.5685027279083639\n",
      "Epoch 92 => Avg Loss = 0.5646696193487162\n",
      "Epoch 93 => Avg Loss = 0.560900357518967\n",
      "Epoch 94 => Avg Loss = 0.5571935328897096\n",
      "Epoch 95 => Avg Loss = 0.5535477462502855\n",
      "Epoch 96 => Avg Loss = 0.5499616110366097\n",
      "Epoch 97 => Avg Loss = 0.546433755410198\n",
      "Epoch 98 => Avg Loss = 0.5429628241037971\n",
      "Epoch 99 => Avg Loss = 0.5395474800488063\n",
      "Epoch 100 => Avg Loss = 0.5361864057993181\n",
      "Epoch 101 => Avg Loss = 0.5328783047671991\n",
      "Epoch 102 => Avg Loss = 0.529621902282116\n",
      "Epoch 103 => Avg Loss = 0.5264159464898831\n",
      "Epoch 104 => Avg Loss = 0.5232592091019052\n",
      "Epoch 105 => Avg Loss = 0.520150486007889\n",
      "Epoch 106 => Avg Loss = 0.5170885977633625\n",
      "Epoch 107 => Avg Loss = 0.5140723899629022\n",
      "Epoch 108 => Avg Loss = 0.5111007335093469\n",
      "Epoch 109 => Avg Loss = 0.5081725247886323\n",
      "Epoch 110 => Avg Loss = 0.5052866857592762\n",
      "Epoch 111 => Avg Loss = 0.5024421639649415\n",
      "Epoch 112 => Avg Loss = 0.4996379324779202\n",
      "Epoch 113 => Avg Loss = 0.4968729897808276\n",
      "Epoch 114 => Avg Loss = 0.49414635959325537\n",
      "Epoch 115 => Avg Loss = 0.4914570906496254\n",
      "Epoch 116 => Avg Loss = 0.4888042564340017\n",
      "Epoch 117 => Avg Loss = 0.4861869548771517\n",
      "Epoch 118 => Avg Loss = 0.4836043080207267\n",
      "Epoch 119 => Avg Loss = 0.48105546165301133\n",
      "Epoch 120 => Avg Loss = 0.4785395849203217\n",
      "Epoch 121 => Avg Loss = 0.4760558699177628\n",
      "Epoch 122 => Avg Loss = 0.473603531262733\n",
      "Epoch 123 => Avg Loss = 0.4711818056542454\n",
      "Epoch 124 => Avg Loss = 0.46878995142085145\n",
      "Epoch 125 => Avg Loss = 0.4664272480596824\n",
      "Epoch 126 => Avg Loss = 0.4640929957688781\n",
      "Epoch 127 => Avg Loss = 0.4617865149754457\n",
      "Epoch 128 => Avg Loss = 0.4595071458603745\n",
      "Epoch 129 => Avg Loss = 0.45725424788264724\n",
      "Epoch 130 => Avg Loss = 0.45502719930360347\n",
      "Epoch 131 => Avg Loss = 0.4528253967129511\n",
      "Epoch 132 => Avg Loss = 0.45064825455756896\n",
      "Epoch 133 => Avg Loss = 0.4484952046741147\n",
      "Epoch 134 => Avg Loss = 0.4463656958263155\n",
      "Epoch 135 => Avg Loss = 0.4442591932477166\n",
      "Epoch 136 => Avg Loss = 0.44217517819055263\n",
      "Epoch 137 => Avg Loss = 0.44011314748131397\n",
      "Epoch 138 => Avg Loss = 0.43807261308350287\n",
      "Epoch 139 => Avg Loss = 0.43605310166797884\n",
      "Epoch 140 => Avg Loss = 0.43405415419124826\n",
      "Epoch 141 => Avg Loss = 0.4320753254819684\n",
      "Epoch 142 => Avg Loss = 0.43011618383589184\n",
      "Epoch 143 => Avg Loss = 0.42817631061942546\n",
      "Epoch 144 => Avg Loss = 0.4262552998819284\n",
      "Epoch 145 => Avg Loss = 0.4243527579768387\n",
      "Epoch 146 => Avg Loss = 0.4224683031916752\n",
      "Epoch 147 => Avg Loss = 0.42060156538694055\n",
      "Epoch 148 => Avg Loss = 0.41875218564390937\n",
      "Epoch 149 => Avg Loss = 0.41691981592127153\n",
      "Epoch 150 => Avg Loss = 0.41510411872057273\n",
      "Epoch 151 => Avg Loss = 0.41330476676037803\n",
      "Epoch 152 => Avg Loss = 0.41152144265906626\n",
      "Epoch 153 => Avg Loss = 0.40975383862614917\n",
      "Epoch 154 => Avg Loss = 0.40800165616199713\n",
      "Epoch 155 => Avg Loss = 0.4062646057658412\n",
      "Epoch 156 => Avg Loss = 0.40454240665191654\n",
      "Epoch 157 => Avg Loss = 0.4028347864735982\n",
      "Epoch 158 => Avg Loss = 0.4011414810553801\n",
      "Epoch 159 => Avg Loss = 0.39946223413254345\n",
      "Epoch 160 => Avg Loss = 0.3977967970983493\n",
      "Epoch 161 => Avg Loss = 0.396144928758598\n",
      "Epoch 162 => Avg Loss = 0.39450639509338764\n",
      "Epoch 163 => Avg Loss = 0.3928809690259078\n",
      "Epoch 164 => Avg Loss = 0.3912684301981007\n",
      "Epoch 165 => Avg Loss = 0.38966856475302364\n",
      "Epoch 166 => Avg Loss = 0.38808116512374957\n",
      "Epoch 167 => Avg Loss = 0.3865060298286352\n",
      "Epoch 168 => Avg Loss = 0.384942963272802\n",
      "Epoch 169 => Avg Loss = 0.38339177555565807\n",
      "Epoch 170 => Avg Loss = 0.3818522822843108\n",
      "Epoch 171 => Avg Loss = 0.3803243043927073\n",
      "Epoch 172 => Avg Loss = 0.37880766796634674\n",
      "Epoch 173 => Avg Loss = 0.37730220407241977\n",
      "Epoch 174 => Avg Loss = 0.3758077485952141\n",
      "Epoch 175 => Avg Loss = 0.37432414207665066\n",
      "Epoch 176 => Avg Loss = 0.3728512295617992\n",
      "Epoch 177 => Avg Loss = 0.37138886044923575\n",
      "Epoch 178 => Avg Loss = 0.369936888346104\n",
      "Epoch 179 => Avg Loss = 0.3684951709277461\n",
      "Epoch 180 => Avg Loss = 0.3670635698017711\n",
      "Epoch 181 => Avg Loss = 0.36564195037643415\n",
      "Epoch 182 => Avg Loss = 0.3642301817331997\n",
      "Epoch 183 => Avg Loss = 0.3628281365033679\n",
      "Epoch 184 => Avg Loss = 0.3614356907486473\n",
      "Epoch 185 => Avg Loss = 0.3600527238455531\n",
      "Epoch 186 => Avg Loss = 0.35867911837352456\n",
      "Epoch 187 => Avg Loss = 0.35731476000664614\n",
      "Epoch 188 => Avg Loss = 0.35595953740887093\n",
      "Epoch 189 => Avg Loss = 0.3546133421326377\n",
      "Epoch 190 => Avg Loss = 0.353276068520785\n",
      "Epoch 191 => Avg Loss = 0.3519476136116598\n",
      "Epoch 192 => Avg Loss = 0.3506288584735157\n",
      "Epoch 193 => Avg Loss = 0.3493194017059251\n",
      "Epoch 194 => Avg Loss = 0.34801841838055797\n",
      "Epoch 195 => Avg Loss = 0.34672583562947396\n",
      "Epoch 196 => Avg Loss = 0.3454415641224058\n",
      "Epoch 197 => Avg Loss = 0.3441655151641125\n",
      "Epoch 198 => Avg Loss = 0.3428976023747883\n",
      "Epoch 199 => Avg Loss = 0.34163774164408556\n",
      "Epoch 200 => Avg Loss = 0.34038585096325896\n",
      "Epoch 201 => Avg Loss = 0.3391418502910006\n",
      "Epoch 202 => Avg Loss = 0.337905661450369\n",
      "Epoch 203 => Avg Loss = 0.3366772080449914\n",
      "Epoch 204 => Avg Loss = 0.33545641538689863\n",
      "Epoch 205 => Avg Loss = 0.3342432104316686\n",
      "Epoch 206 => Avg Loss = 0.33303752171846523\n",
      "Epoch 207 => Avg Loss = 0.33183927931362145\n",
      "Epoch 208 => Avg Loss = 0.3306484147569856\n",
      "Epoch 209 => Avg Loss = 0.3294648610105829\n",
      "Epoch 210 => Avg Loss = 0.32828855240930743\n",
      "Epoch 211 => Avg Loss = 0.3271194246134697\n",
      "Epoch 212 => Avg Loss = 0.32595853626756705\n",
      "Epoch 213 => Avg Loss = 0.3248055338978676\n",
      "Epoch 214 => Avg Loss = 0.323659472953582\n",
      "Epoch 215 => Avg Loss = 0.3225203242359669\n",
      "Epoch 216 => Avg Loss = 0.3213880311016469\n",
      "Epoch 217 => Avg Loss = 0.32026253541843214\n",
      "Epoch 218 => Avg Loss = 0.31914378035447794\n",
      "Epoch 219 => Avg Loss = 0.31803171044105627\n",
      "Epoch 220 => Avg Loss = 0.31692627140680313\n",
      "Epoch 221 => Avg Loss = 0.31582741004716597\n",
      "Epoch 222 => Avg Loss = 0.31473507413421903\n",
      "Epoch 223 => Avg Loss = 0.31364921235221394\n",
      "Epoch 224 => Avg Loss = 0.3125697742485529\n",
      "Epoch 225 => Avg Loss = 0.3114967101941041\n",
      "Epoch 226 => Avg Loss = 0.31042997134934863\n",
      "Epoch 227 => Avg Loss = 0.3093695096343264\n",
      "Epoch 228 => Avg Loss = 0.3083152777011834\n",
      "Epoch 229 => Avg Loss = 0.3072670796962606\n",
      "Epoch 230 => Avg Loss = 0.3062260531214726\n",
      "Epoch 231 => Avg Loss = 0.3051910175959904\n",
      "Epoch 232 => Avg Loss = 0.3041620070220043\n",
      "Epoch 233 => Avg Loss = 0.3031389866268141\n",
      "Epoch 234 => Avg Loss = 0.302121913622691\n",
      "Epoch 235 => Avg Loss = 0.30111074496825957\n",
      "Epoch 236 => Avg Loss = 0.3001054382498788\n",
      "Epoch 237 => Avg Loss = 0.299105951709025\n",
      "Epoch 238 => Avg Loss = 0.29811224418779925\n",
      "Epoch 239 => Avg Loss = 0.297124275081776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240 => Avg Loss = 0.2961420043047313\n",
      "Epoch 241 => Avg Loss = 0.2951653922616364\n",
      "Epoch 242 => Avg Loss = 0.2941943998269547\n",
      "Epoch 243 => Avg Loss = 0.2932289883263656\n",
      "Epoch 244 => Avg Loss = 0.29226911952077\n",
      "Epoch 245 => Avg Loss = 0.29131475559187936\n",
      "Epoch 246 => Avg Loss = 0.2903658591289569\n",
      "Epoch 247 => Avg Loss = 0.289422393116442\n",
      "Epoch 248 => Avg Loss = 0.2884843209222924\n",
      "Epoch 249 => Avg Loss = 0.28755160628692755\n",
      "Epoch 250 => Avg Loss = 0.2866236321508983\n",
      "Epoch 251 => Avg Loss = 0.28570296177581983\n",
      "Epoch 252 => Avg Loss = 0.2847880256377231\n",
      "Epoch 253 => Avg Loss = 0.28387807729919234\n",
      "Epoch 254 => Avg Loss = 0.28297326864020655\n",
      "Epoch 255 => Avg Loss = 0.282073588179817\n",
      "Epoch 256 => Avg Loss = 0.2811790034780805\n",
      "Epoch 257 => Avg Loss = 0.280289480187248\n",
      "Epoch 258 => Avg Loss = 0.2794049843708358\n",
      "Epoch 259 => Avg Loss = 0.27852548264844257\n",
      "Epoch 260 => Avg Loss = 0.2776509421111033\n",
      "Epoch 261 => Avg Loss = 0.2767813302447092\n",
      "Epoch 262 => Avg Loss = 0.27591661487781266\n",
      "Epoch 263 => Avg Loss = 0.2750567641468177\n",
      "Epoch 264 => Avg Loss = 0.2742017464722068\n",
      "Epoch 265 => Avg Loss = 0.2733515305417126\n",
      "Epoch 266 => Avg Loss = 0.27250596783432796\n",
      "Epoch 267 => Avg Loss = 0.27166716356958376\n",
      "Epoch 268 => Avg Loss = 0.2708323447671572\n",
      "Epoch 269 => Avg Loss = 0.2700021257860669\n",
      "Epoch 270 => Avg Loss = 0.26917654117914636\n",
      "Epoch 271 => Avg Loss = 0.26835556787757764\n",
      "Epoch 272 => Avg Loss = 0.26753917574898245\n",
      "Epoch 273 => Avg Loss = 0.26672733438556534\n",
      "Epoch 274 => Avg Loss = 0.26592001385049024\n",
      "Epoch 275 => Avg Loss = 0.2651171846554779\n",
      "Epoch 276 => Avg Loss = 0.26431881767903165\n",
      "Epoch 277 => Avg Loss = 0.2635248841052889\n",
      "Epoch 278 => Avg Loss = 0.2627353553832958\n",
      "Epoch 279 => Avg Loss = 0.26195110232437374\n",
      "Epoch 280 => Avg Loss = 0.2611706799492326\n",
      "Epoch 281 => Avg Loss = 0.260394527882826\n",
      "Epoch 282 => Avg Loss = 0.2596226622887578\n",
      "Epoch 283 => Avg Loss = 0.2588550610267886\n",
      "Epoch 284 => Avg Loss = 0.25809169690881256\n",
      "Epoch 285 => Avg Loss = 0.25733254239031844\n",
      "Epoch 286 => Avg Loss = 0.256577570169254\n",
      "Epoch 287 => Avg Loss = 0.2558267532208561\n",
      "Epoch 288 => Avg Loss = 0.255080064768935\n",
      "Epoch 289 => Avg Loss = 0.25433747825940134\n",
      "Epoch 290 => Avg Loss = 0.2535989673414033\n",
      "Epoch 291 => Avg Loss = 0.2528645058542606\n",
      "Epoch 292 => Avg Loss = 0.2521340678183022\n",
      "Epoch 293 => Avg Loss = 0.2514076274282863\n",
      "Epoch 294 => Avg Loss = 0.25068515904854205\n",
      "Epoch 295 => Avg Loss = 0.24996663720926338\n",
      "Epoch 296 => Avg Loss = 0.24925203660358408\n",
      "Epoch 297 => Avg Loss = 0.24854133208519094\n",
      "Epoch 298 => Avg Loss = 0.24783449866631016\n",
      "Epoch 299 => Avg Loss = 0.24713151151595658\n",
      "Epoch 300 => Avg Loss = 0.24643234595837757\n",
      "Epoch 301 => Avg Loss = 0.2457369774716351\n",
      "Epoch 302 => Avg Loss = 0.24504538168629833\n",
      "Epoch 303 => Avg Loss = 0.2443575343842201\n",
      "Epoch 304 => Avg Loss = 0.24367341149738114\n",
      "Epoch 305 => Avg Loss = 0.24299298910679185\n",
      "Epoch 306 => Avg Loss = 0.24231624344144187\n",
      "Epoch 307 => Avg Loss = 0.2416431508772921\n",
      "Epoch 308 => Avg Loss = 0.24097368793630386\n",
      "Epoch 309 => Avg Loss = 0.24030783128550254\n",
      "Epoch 310 => Avg Loss = 0.23964555773607002\n",
      "Epoch 311 => Avg Loss = 0.23898684424246755\n",
      "Epoch 312 => Avg Loss = 0.2383316679015826\n",
      "Epoch 313 => Avg Loss = 0.23768000595190125\n",
      "Epoch 314 => Avg Loss = 0.23703183577270207\n",
      "Epoch 315 => Avg Loss = 0.23638713488327073\n",
      "Epoch 316 => Avg Loss = 0.23574588094213525\n",
      "Epoch 317 => Avg Loss = 0.23510805174631805\n",
      "Epoch 318 => Avg Loss = 0.23447362523060533\n",
      "Epoch 319 => Avg Loss = 0.23384257946683235\n",
      "Epoch 320 => Avg Loss = 0.23321489266318343\n",
      "Epoch 321 => Avg Loss = 0.2325905431635056\n",
      "Epoch 322 => Avg Loss = 0.23196950944663414\n",
      "Epoch 323 => Avg Loss = 0.2313514689428274\n",
      "Epoch 324 => Avg Loss = 0.23073744122567202\n",
      "Epoch 325 => Avg Loss = 0.2301264612518454\n",
      "Epoch 326 => Avg Loss = 0.2295186820597127\n",
      "Epoch 327 => Avg Loss = 0.22891410934719153\n",
      "Epoch 328 => Avg Loss = 0.2283127259515476\n",
      "Epoch 329 => Avg Loss = 0.2277145114039779\n",
      "Epoch 330 => Avg Loss = 0.22711944495376057\n",
      "Epoch 331 => Avg Loss = 0.22652757475081628\n",
      "Epoch 332 => Avg Loss = 0.2259390499232856\n",
      "Epoch 333 => Avg Loss = 0.22535344934388968\n",
      "Epoch 334 => Avg Loss = 0.22477089348844714\n",
      "Epoch 335 => Avg Loss = 0.2241913839089239\n",
      "Epoch 336 => Avg Loss = 0.22361490355776\n",
      "Epoch 337 => Avg Loss = 0.22304143273979313\n",
      "Epoch 338 => Avg Loss = 0.22247095159355854\n",
      "Epoch 339 => Avg Loss = 0.22190344044018712\n",
      "Epoch 340 => Avg Loss = 0.22133887980669129\n",
      "Epoch 341 => Avg Loss = 0.22077725040753762\n",
      "Epoch 342 => Avg Loss = 0.22021853312643408\n",
      "Epoch 343 => Avg Loss = 0.21966270900275037\n",
      "Epoch 344 => Avg Loss = 0.2191097592217853\n",
      "Epoch 345 => Avg Loss = 0.2185596651077742\n",
      "Epoch 346 => Avg Loss = 0.21801240811879852\n",
      "Epoch 347 => Avg Loss = 0.21746796984302091\n",
      "Epoch 348 => Avg Loss = 0.2169263319958455\n",
      "Epoch 349 => Avg Loss = 0.2163874764177277\n",
      "Epoch 350 => Avg Loss = 0.21585138507244475\n",
      "Epoch 351 => Avg Loss = 0.21531804004568902\n",
      "Epoch 352 => Avg Loss = 0.21478742354389352\n",
      "Epoch 353 => Avg Loss = 0.2142595178932206\n",
      "Epoch 354 => Avg Loss = 0.2137343055386684\n",
      "Epoch 355 => Avg Loss = 0.21321176904326145\n",
      "Epoch 356 => Avg Loss = 0.2126918910872995\n",
      "Epoch 357 => Avg Loss = 0.21217465446765138\n",
      "Epoch 358 => Avg Loss = 0.2116600420970788\n",
      "Epoch 359 => Avg Loss = 0.2111480370035817\n",
      "Epoch 360 => Avg Loss = 0.21063862232976183\n",
      "Epoch 361 => Avg Loss = 0.21013178133219465\n",
      "Epoch 362 => Avg Loss = 0.2096274973808134\n",
      "Epoch 363 => Avg Loss = 0.20912575395829577\n",
      "Epoch 364 => Avg Loss = 0.20862653465945755\n",
      "Epoch 365 => Avg Loss = 0.20812982319064757\n",
      "Epoch 366 => Avg Loss = 0.20763560336914674\n",
      "Epoch 367 => Avg Loss = 0.2071438591225671\n",
      "Epoch 368 => Avg Loss = 0.20665457448825394\n",
      "Epoch 369 => Avg Loss = 0.20616773361268687\n",
      "Epoch 370 => Avg Loss = 0.2056833207508838\n",
      "Epoch 371 => Avg Loss = 0.20520132026580343\n",
      "Epoch 372 => Avg Loss = 0.20472171662774968\n",
      "Epoch 373 => Avg Loss = 0.2042444944137756\n",
      "Epoch 374 => Avg Loss = 0.20376963830708789\n",
      "Epoch 375 => Avg Loss = 0.2032971330964523\n",
      "Epoch 376 => Avg Loss = 0.2028269636755981\n",
      "Epoch 377 => Avg Loss = 0.20235911504262394\n",
      "Epoch 378 => Avg Loss = 0.20189357229940327\n",
      "Epoch 379 => Avg Loss = 0.2014303206509906\n",
      "Epoch 380 => Avg Loss = 0.2009693454050278\n",
      "Epoch 381 => Avg Loss = 0.20051063197115052\n",
      "Epoch 382 => Avg Loss = 0.20005416586039573\n",
      "Epoch 383 => Avg Loss = 0.19959993268460913\n",
      "Epoch 384 => Avg Loss = 0.1991479181558526\n",
      "Epoch 385 => Avg Loss = 0.19869810808581376\n",
      "Epoch 386 => Avg Loss = 0.19825048838521345\n",
      "Epoch 387 => Avg Loss = 0.19780504506321703\n",
      "Epoch 388 => Avg Loss = 0.1973617642268433\n",
      "Epoch 389 => Avg Loss = 0.19692063208037583\n",
      "Epoch 390 => Avg Loss = 0.1964816349247744\n",
      "Epoch 391 => Avg Loss = 0.19604475915708783\n",
      "Epoch 392 => Avg Loss = 0.19560999126986622\n",
      "Epoch 393 => Avg Loss = 0.1951773178505755\n",
      "Epoch 394 => Avg Loss = 0.19474672558101275\n",
      "Epoch 395 => Avg Loss = 0.19431820123672067\n",
      "Epoch 396 => Avg Loss = 0.1938917316864058\n",
      "Epoch 397 => Avg Loss = 0.1934673038913553\n",
      "Epoch 398 => Avg Loss = 0.1930449049048565\n",
      "Epoch 399 => Avg Loss = 0.1926245218716165\n",
      "Epoch 400 => Avg Loss = 0.19220614202718364\n",
      "Epoch 401 => Avg Loss = 0.19178975269737025\n",
      "Epoch 402 => Avg Loss = 0.1913753412976755\n",
      "Epoch 403 => Avg Loss = 0.1909631515681711\n",
      "Epoch 404 => Avg Loss = 0.1905528125361934\n",
      "Epoch 405 => Avg Loss = 0.19014439623581997\n",
      "Epoch 406 => Avg Loss = 0.1897379039579547\n",
      "Epoch 407 => Avg Loss = 0.18933332598543715\n",
      "Epoch 408 => Avg Loss = 0.18893065068911677\n",
      "Epoch 409 => Avg Loss = 0.1885298661698247\n",
      "Epoch 410 => Avg Loss = 0.18813096055362316\n",
      "Epoch 411 => Avg Loss = 0.18773392204369832\n",
      "Epoch 412 => Avg Loss = 0.187338738928437\n",
      "Epoch 413 => Avg Loss = 0.18694536223362\n",
      "Epoch 414 => Avg Loss = 0.18655399017180926\n",
      "Epoch 415 => Avg Loss = 0.18616440501574041\n",
      "Epoch 416 => Avg Loss = 0.1857766221640204\n",
      "Epoch 417 => Avg Loss = 0.18539063522743965\n",
      "Epoch 418 => Avg Loss = 0.18500643388704313\n",
      "Epoch 419 => Avg Loss = 0.1846240071713883\n",
      "Epoch 420 => Avg Loss = 0.18424334405742476\n",
      "Epoch 421 => Avg Loss = 0.18386443357972723\n",
      "Epoch 422 => Avg Loss = 0.1834872648494609\n",
      "Epoch 423 => Avg Loss = 0.18311182705688683\n",
      "Epoch 424 => Avg Loss = 0.1827381094709671\n",
      "Epoch 425 => Avg Loss = 0.18236610143853543\n",
      "Epoch 426 => Avg Loss = 0.1819957923834687\n",
      "Epoch 427 => Avg Loss = 0.18162717180591642\n",
      "Epoch 428 => Avg Loss = 0.18126022928159038\n",
      "Epoch 429 => Avg Loss = 0.18089495446109755\n",
      "Epoch 430 => Avg Loss = 0.18053133706931074\n",
      "Epoch 431 => Avg Loss = 0.18016936690476798\n",
      "Epoch 432 => Avg Loss = 0.17980903383909122\n",
      "Epoch 433 => Avg Loss = 0.1794503278164254\n",
      "Epoch 434 => Avg Loss = 0.17909323885288952\n",
      "Epoch 435 => Avg Loss = 0.17873775703603914\n",
      "Epoch 436 => Avg Loss = 0.1783838725243381\n",
      "Epoch 437 => Avg Loss = 0.17803157554663807\n",
      "Epoch 438 => Avg Loss = 0.17768085640166373\n",
      "Epoch 439 => Avg Loss = 0.17733170545750426\n",
      "Epoch 440 => Avg Loss = 0.17698411315110907\n",
      "Epoch 441 => Avg Loss = 0.17663806998778753\n",
      "Epoch 442 => Avg Loss = 0.17629356654071326\n",
      "Epoch 443 => Avg Loss = 0.17595059345043162\n",
      "Epoch 444 => Avg Loss = 0.17560914142437056\n",
      "Epoch 445 => Avg Loss = 0.17526920123635428\n",
      "Epoch 446 => Avg Loss = 0.17493076372612118\n",
      "Epoch 447 => Avg Loss = 0.17459381979884253\n",
      "Epoch 448 => Avg Loss = 0.17425836042464668\n",
      "Epoch 449 => Avg Loss = 0.17392437663814383\n",
      "Epoch 450 => Avg Loss = 0.17359185953795475\n",
      "Epoch 451 => Avg Loss = 0.1732608002862421\n",
      "Epoch 452 => Avg Loss = 0.17293119010824434\n",
      "Epoch 453 => Avg Loss = 0.1726030202918119\n",
      "Epoch 454 => Avg Loss = 0.17227628218694768\n",
      "Epoch 455 => Avg Loss = 0.17195096720534747\n",
      "Epoch 456 => Avg Loss = 0.17162706681994627\n",
      "Epoch 457 => Avg Loss = 0.1713045725644645\n",
      "Epoch 458 => Avg Loss = 0.17098347603295905\n",
      "Epoch 459 => Avg Loss = 0.17066376887937526\n",
      "Epoch 460 => Avg Loss = 0.17034544281710326\n",
      "Epoch 461 => Avg Loss = 0.17002852781640426\n",
      "Epoch 462 => Avg Loss = 0.16971314867444756\n",
      "Epoch 463 => Avg Loss = 0.16939905955150178\n",
      "Epoch 464 => Avg Loss = 0.1690863049481265\n",
      "Epoch 465 => Avg Loss = 0.16877488719681494\n",
      "Epoch 466 => Avg Loss = 0.16846480037892644\n",
      "Epoch 467 => Avg Loss = 0.1681560370002561\n",
      "Epoch 468 => Avg Loss = 0.16784858930486166\n",
      "Epoch 469 => Avg Loss = 0.1675424495329635\n",
      "Epoch 470 => Avg Loss = 0.16723760997080142\n",
      "Epoch 471 => Avg Loss = 0.16693406295957622\n",
      "Epoch 472 => Avg Loss = 0.16663180089642898\n",
      "Epoch 473 => Avg Loss = 0.1663308162339491\n",
      "Epoch 474 => Avg Loss = 0.16603110147947125\n",
      "Epoch 475 => Avg Loss = 0.16573264919439182\n",
      "Epoch 476 => Avg Loss = 0.1654354519935402\n",
      "Epoch 477 => Avg Loss = 0.1651395025445995\n",
      "Epoch 478 => Avg Loss = 0.16484479356756884\n",
      "Epoch 479 => Avg Loss = 0.16455131783425883\n",
      "Epoch 480 => Avg Loss = 0.16425906816781222\n",
      "Epoch 481 => Avg Loss = 0.16396803744224855\n",
      "Epoch 482 => Avg Loss = 0.16367821858202325\n",
      "Epoch 483 => Avg Loss = 0.16338960456160348\n",
      "Epoch 484 => Avg Loss = 0.16310254007865577\n",
      "Epoch 485 => Avg Loss = 0.1628160247706233\n",
      "Epoch 486 => Avg Loss = 0.16253169377213703\n",
      "Epoch 487 => Avg Loss = 0.16224808955496292\n",
      "Epoch 488 => Avg Loss = 0.16196587425194794\n",
      "Epoch 489 => Avg Loss = 0.16168513204833926\n",
      "Epoch 490 => Avg Loss = 0.1614053489279247\n",
      "Epoch 491 => Avg Loss = 0.16112667329796046\n",
      "Epoch 492 => Avg Loss = 0.16084912991706365\n",
      "Epoch 493 => Avg Loss = 0.16057271840596513\n",
      "Epoch 494 => Avg Loss = 0.1602974333501087\n",
      "Epoch 495 => Avg Loss = 0.16002326838874723\n",
      "Epoch 496 => Avg Loss = 0.15975009557199188\n",
      "Epoch 497 => Avg Loss = 0.15947834607752678\n",
      "Epoch 498 => Avg Loss = 0.1592076364380193\n",
      "Epoch 499 => Avg Loss = 0.15893800794517784\n",
      "Epoch 500 => Avg Loss = 0.15866946413251334\n",
      "Epoch 501 => Avg Loss = 0.15840200080257366\n",
      "Epoch 502 => Avg Loss = 0.15813561221026004\n",
      "Epoch 503 => Avg Loss = 0.1578702923347325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 504 => Avg Loss = 0.15760603513951102\n",
      "Epoch 505 => Avg Loss = 0.15734283462426524\n",
      "Epoch 506 => Avg Loss = 0.15708068483394463\n",
      "Epoch 507 => Avg Loss = 0.1568195798593713\n",
      "Epoch 508 => Avg Loss = 0.15655951383629227\n",
      "Epoch 509 => Avg Loss = 0.15630048094428578\n",
      "Epoch 510 => Avg Loss = 0.1560424754057733\n",
      "Epoch 511 => Avg Loss = 0.15578549148516274\n",
      "Epoch 512 => Avg Loss = 0.15552941811184076\n",
      "Epoch 513 => Avg Loss = 0.15527464222389176\n",
      "Epoch 514 => Avg Loss = 0.15502084571928695\n",
      "Epoch 515 => Avg Loss = 0.15476903970292227\n",
      "Epoch 516 => Avg Loss = 0.15451630560562116\n",
      "Epoch 517 => Avg Loss = 0.1542664280618476\n",
      "Epoch 518 => Avg Loss = 0.1540163665663515\n",
      "Epoch 519 => Avg Loss = 0.1537666717541103\n",
      "Epoch 520 => Avg Loss = 0.15351981267355308\n",
      "Epoch 521 => Avg Loss = 0.15327303441921242\n",
      "Epoch 522 => Avg Loss = 0.15302615489766372\n",
      "Epoch 523 => Avg Loss = 0.15278240823868192\n",
      "Epoch 524 => Avg Loss = 0.15253876353561027\n",
      "Epoch 525 => Avg Loss = 0.1522949904293151\n",
      "Epoch 526 => Avg Loss = 0.15205402522349604\n",
      "Epoch 527 => Avg Loss = 0.15181311923730006\n",
      "Epoch 528 => Avg Loss = 0.15157212743224216\n",
      "Epoch 529 => Avg Loss = 0.15133417580667102\n",
      "Epoch 530 => Avg Loss = 0.15109610724214936\n",
      "Epoch 531 => Avg Loss = 0.150859338074759\n",
      "Epoch 532 => Avg Loss = 0.1506222791910634\n",
      "Epoch 533 => Avg Loss = 0.15038798799081815\n",
      "Epoch 534 => Avg Loss = 0.15015368996512887\n",
      "Epoch 535 => Avg Loss = 0.14992048856885726\n",
      "Epoch 536 => Avg Loss = 0.1496882864396532\n",
      "Epoch 537 => Avg Loss = 0.14945683949889815\n",
      "Epoch 538 => Avg Loss = 0.14922528804667\n",
      "Epoch 539 => Avg Loss = 0.14899646266266403\n",
      "Epoch 540 => Avg Loss = 0.14876806061023062\n",
      "Epoch 541 => Avg Loss = 0.1485401603055234\n",
      "Epoch 542 => Avg Loss = 0.14831308679304614\n",
      "Epoch 543 => Avg Loss = 0.14808685727514287\n",
      "Epoch 544 => Avg Loss = 0.1478614718899454\n",
      "Epoch 545 => Avg Loss = 0.14763695273883257\n",
      "Epoch 546 => Avg Loss = 0.1474133262617587\n",
      "Epoch 547 => Avg Loss = 0.1471905126642067\n",
      "Epoch 548 => Avg Loss = 0.14696757429368051\n",
      "Epoch 549 => Avg Loss = 0.14674729647447088\n",
      "Epoch 550 => Avg Loss = 0.14652699169042296\n",
      "Epoch 551 => Avg Loss = 0.14630746425158905\n",
      "Epoch 552 => Avg Loss = 0.14608873854150817\n",
      "Epoch 553 => Avg Loss = 0.1458708165650645\n",
      "Epoch 554 => Avg Loss = 0.14565369545310244\n",
      "Epoch 555 => Avg Loss = 0.1454373712949369\n",
      "Epoch 556 => Avg Loss = 0.14522183997712973\n",
      "Epoch 557 => Avg Loss = 0.14500709736651338\n",
      "Epoch 558 => Avg Loss = 0.14479337300525855\n",
      "Epoch 559 => Avg Loss = 0.14458025641990468\n",
      "Epoch 560 => Avg Loss = 0.1443679043931347\n",
      "Epoch 561 => Avg Loss = 0.1441563218649353\n",
      "Epoch 562 => Avg Loss = 0.1439455068116195\n",
      "Epoch 563 => Avg Loss = 0.14373587885747066\n",
      "Epoch 564 => Avg Loss = 0.1435266685088085\n",
      "Epoch 565 => Avg Loss = 0.1433181365808494\n",
      "Epoch 566 => Avg Loss = 0.14311055210064977\n",
      "Epoch 567 => Avg Loss = 0.14290365088270227\n",
      "Epoch 568 => Avg Loss = 0.14269747908184705\n",
      "Epoch 569 => Avg Loss = 0.14249204400777143\n",
      "Epoch 570 => Avg Loss = 0.14228734433677256\n",
      "Epoch 571 => Avg Loss = 0.14208337684355868\n",
      "Epoch 572 => Avg Loss = 0.14188013790090423\n",
      "Epoch 573 => Avg Loss = 0.14167762381342314\n",
      "Epoch 574 => Avg Loss = 0.1414758308914277\n",
      "Epoch 575 => Avg Loss = 0.14127475546686422\n",
      "Epoch 576 => Avg Loss = 0.14107439389638574\n",
      "Epoch 577 => Avg Loss = 0.14087474256161417\n",
      "Epoch 578 => Avg Loss = 0.14067579786882753\n",
      "Epoch 579 => Avg Loss = 0.14047755624856176\n",
      "Epoch 580 => Avg Loss = 0.14027989658113615\n",
      "Epoch 581 => Avg Loss = 0.14008317044637747\n",
      "Epoch 582 => Avg Loss = 0.1398871705936335\n",
      "Epoch 583 => Avg Loss = 0.13969185161158115\n",
      "Epoch 584 => Avg Loss = 0.13949718266918024\n",
      "Epoch 585 => Avg Loss = 0.1393031872962859\n",
      "Epoch 586 => Avg Loss = 0.1391098681792957\n",
      "Epoch 587 => Avg Loss = 0.1389172233006097\n",
      "Epoch 588 => Avg Loss = 0.13872524959567512\n",
      "Epoch 589 => Avg Loss = 0.13853394378152992\n",
      "Epoch 590 => Avg Loss = 0.13834330254422372\n",
      "Epoch 591 => Avg Loss = 0.13815374244202622\n",
      "Epoch 592 => Avg Loss = 0.1379644871192604\n",
      "Epoch 593 => Avg Loss = 0.1377758672727009\n",
      "Epoch 594 => Avg Loss = 0.13758789430757487\n",
      "Epoch 595 => Avg Loss = 0.13740056834689782\n",
      "Epoch 596 => Avg Loss = 0.1372138869521251\n",
      "Epoch 597 => Avg Loss = 0.13702784711614824\n",
      "Epoch 598 => Avg Loss = 0.13684244571873272\n",
      "Epoch 599 => Avg Loss = 0.13665767963037778\n",
      "Epoch 600 => Avg Loss = 0.13647354573575476\n",
      "Epoch 601 => Avg Loss = 0.13629004093876285\n",
      "Epoch 602 => Avg Loss = 0.13610716216340837\n",
      "Epoch 603 => Avg Loss = 0.1359249063537543\n",
      "Epoch 604 => Avg Loss = 0.13574326882830168\n",
      "Epoch 605 => Avg Loss = 0.1355623156425458\n",
      "Epoch 606 => Avg Loss = 0.1353817253301542\n",
      "Epoch 607 => Avg Loss = 0.1352020619064326\n",
      "Epoch 608 => Avg Loss = 0.13502296107839779\n",
      "Epoch 609 => Avg Loss = 0.13484445442265677\n",
      "Epoch 610 => Avg Loss = 0.13466654701401015\n",
      "Epoch 611 => Avg Loss = 0.13448923779409733\n",
      "Epoch 612 => Avg Loss = 0.13431252429704219\n",
      "Epoch 613 => Avg Loss = 0.13413640374572153\n",
      "Epoch 614 => Avg Loss = 0.13396087330593642\n",
      "Epoch 615 => Avg Loss = 0.13378593014510257\n",
      "Epoch 616 => Avg Loss = 0.13361157144555894\n",
      "Epoch 617 => Avg Loss = 0.13343779440736395\n",
      "Epoch 618 => Avg Loss = 0.13326459624868087\n",
      "Epoch 619 => Avg Loss = 0.13309184736497884\n",
      "Epoch 620 => Avg Loss = 0.13291986262498673\n",
      "Epoch 621 => Avg Loss = 0.13274842192667055\n",
      "Epoch 622 => Avg Loss = 0.13257754279890197\n",
      "Epoch 623 => Avg Loss = 0.13240722729208582\n",
      "Epoch 624 => Avg Loss = 0.13223747383643472\n",
      "Epoch 625 => Avg Loss = 0.13206819871725198\n",
      "Epoch 626 => Avg Loss = 0.1318996293437689\n",
      "Epoch 627 => Avg Loss = 0.1317316737451086\n",
      "Epoch 628 => Avg Loss = 0.1315642321609055\n",
      "Epoch 629 => Avg Loss = 0.13139730865393204\n",
      "Epoch 630 => Avg Loss = 0.13123092422415988\n",
      "Epoch 631 => Avg Loss = 0.1310650109265662\n",
      "Epoch 632 => Avg Loss = 0.13089974414905434\n",
      "Epoch 633 => Avg Loss = 0.13073500152971873\n",
      "Epoch 634 => Avg Loss = 0.13057079157388768\n",
      "Epoch 635 => Avg Loss = 0.1304071143858567\n",
      "Epoch 636 => Avg Loss = 0.13024396808966865\n",
      "Epoch 637 => Avg Loss = 0.1300813503522064\n",
      "Epoch 638 => Avg Loss = 0.12991925874444626\n",
      "Epoch 639 => Avg Loss = 0.1297576908269758\n",
      "Epoch 640 => Avg Loss = 0.12959664417004796\n",
      "Epoch 641 => Avg Loss = 0.12943611635810665\n",
      "Epoch 642 => Avg Loss = 0.1292761049906441\n",
      "Epoch 643 => Avg Loss = 0.12911660768220953\n",
      "Epoch 644 => Avg Loss = 0.1289576220622349\n",
      "Epoch 645 => Avg Loss = 0.12879914577483398\n",
      "Epoch 646 => Avg Loss = 0.12864117647860907\n",
      "Epoch 647 => Avg Loss = 0.12848371184647056\n",
      "Epoch 648 => Avg Loss = 0.12832674956546997\n",
      "Epoch 649 => Avg Loss = 0.12817028733664484\n",
      "Epoch 650 => Avg Loss = 0.12801432287487258\n",
      "Epoch 651 => Avg Loss = 0.12785885390873333\n",
      "Epoch 652 => Avg Loss = 0.12770387818038018\n",
      "Epoch 653 => Avg Loss = 0.12754939344541497\n",
      "Epoch 654 => Avg Loss = 0.12739539747276923\n",
      "Epoch 655 => Avg Loss = 0.127241888044591\n",
      "Epoch 656 => Avg Loss = 0.127088862956134\n",
      "Epoch 657 => Avg Loss = 0.12693632001565083\n",
      "Epoch 658 => Avg Loss = 0.12678425704429028\n",
      "Epoch 659 => Avg Loss = 0.12663267187599553\n",
      "Epoch 660 => Avg Loss = 0.12648156235740562\n",
      "Epoch 661 => Avg Loss = 0.1263309263477596\n",
      "Epoch 662 => Avg Loss = 0.12618076171880116\n",
      "Epoch 663 => Avg Loss = 0.12603106635468647\n",
      "Epoch 664 => Avg Loss = 0.1258815910839761\n",
      "Epoch 665 => Avg Loss = 0.12573291114767693\n",
      "Epoch 666 => Avg Loss = 0.1255846410843527\n",
      "Epoch 667 => Avg Loss = 0.1254368189466374\n",
      "Epoch 668 => Avg Loss = 0.1252894524773496\n",
      "Epoch 669 => Avg Loss = 0.12514254202190095\n",
      "Epoch 670 => Avg Loss = 0.12499608612781163\n",
      "Epoch 671 => Avg Loss = 0.12485008291264003\n",
      "Epoch 672 => Avg Loss = 0.12470453039820842\n",
      "Epoch 673 => Avg Loss = 0.12455942659220232\n",
      "Epoch 674 => Avg Loss = 0.12441476950799765\n",
      "Epoch 675 => Avg Loss = 0.12427055716938068\n",
      "Epoch 676 => Avg Loss = 0.12412678761158098\n",
      "Epoch 677 => Avg Loss = 0.12398345888140964\n",
      "Epoch 678 => Avg Loss = 0.12384056903718739\n",
      "Epoch 679 => Avg Loss = 0.12369811614862912\n",
      "Epoch 680 => Avg Loss = 0.12355609829672373\n",
      "Epoch 681 => Avg Loss = 0.12341451357361853\n",
      "Epoch 682 => Avg Loss = 0.12327336008250958\n",
      "Epoch 683 => Avg Loss = 0.12313263593753764\n",
      "Epoch 684 => Avg Loss = 0.12299233926368895\n",
      "Epoch 685 => Avg Loss = 0.12285246819670069\n",
      "Epoch 686 => Avg Loss = 0.12271302088296904\n",
      "Epoch 687 => Avg Loss = 0.12257399547946192\n",
      "Epoch 688 => Avg Loss = 0.12243539015363417\n",
      "Epoch 689 => Avg Loss = 0.12229720308334428\n",
      "Epoch 690 => Avg Loss = 0.12215943245677477\n",
      "Epoch 691 => Avg Loss = 0.12202207647235329\n",
      "Epoch 692 => Avg Loss = 0.12188513333867648\n",
      "Epoch 693 => Avg Loss = 0.12174860127443493\n",
      "Epoch 694 => Avg Loss = 0.12161247850833942\n",
      "Epoch 695 => Avg Loss = 0.12147676327904884\n",
      "Epoch 696 => Avg Loss = 0.12134145383509876\n",
      "Epoch 697 => Avg Loss = 0.1212065484348319\n",
      "Epoch 698 => Avg Loss = 0.12107204534632851\n",
      "Epoch 699 => Avg Loss = 0.12093794284733858\n",
      "Epoch 700 => Avg Loss = 0.12080423922521483\n",
      "Epoch 701 => Avg Loss = 0.12067093277684551\n",
      "Epoch 702 => Avg Loss = 0.12053802180858963\n",
      "Epoch 703 => Avg Loss = 0.12040550463621104\n",
      "Epoch 704 => Avg Loss = 0.12027337958481489\n",
      "Epoch 705 => Avg Loss = 0.12014164957115736\n",
      "Epoch 706 => Avg Loss = 0.1200103432593206\n",
      "Epoch 707 => Avg Loss = 0.11987940422192728\n",
      "Epoch 708 => Avg Loss = 0.11974884579679206\n",
      "Epoch 709 => Avg Loss = 0.11961867011249479\n",
      "Epoch 710 => Avg Loss = 0.11948887647857008\n",
      "Epoch 711 => Avg Loss = 0.11935946350230728\n",
      "Epoch 712 => Avg Loss = 0.11923043233116472\n",
      "Epoch 713 => Avg Loss = 0.11910182845834252\n",
      "Epoch 714 => Avg Loss = 0.11897356766148091\n",
      "Epoch 715 => Avg Loss = 0.11884567283351857\n",
      "Epoch 716 => Avg Loss = 0.11871814860756061\n",
      "Epoch 717 => Avg Loss = 0.11859099498945724\n",
      "Epoch 718 => Avg Loss = 0.11846421081809645\n",
      "Epoch 719 => Avg Loss = 0.11833767421080471\n",
      "Epoch 720 => Avg Loss = 0.11821164420720495\n",
      "Epoch 721 => Avg Loss = 0.11808596585452907\n",
      "Epoch 722 => Avg Loss = 0.11796076909436236\n",
      "Epoch 723 => Avg Loss = 0.11783567428348714\n",
      "Epoch 724 => Avg Loss = 0.11771109082441661\n",
      "Epoch 725 => Avg Loss = 0.11758685648410223\n",
      "Epoch 726 => Avg Loss = 0.11746297717829773\n",
      "Epoch 727 => Avg Loss = 0.11733945330838695\n",
      "Epoch 728 => Avg Loss = 0.117216283873349\n",
      "Epoch 729 => Avg Loss = 0.1170935868692108\n",
      "Epoch 730 => Avg Loss = 0.11697098662189814\n",
      "Epoch 731 => Avg Loss = 0.11684881044307746\n",
      "Epoch 732 => Avg Loss = 0.11672708575075544\n",
      "Epoch 733 => Avg Loss = 0.11660568304282383\n",
      "Epoch 734 => Avg Loss = 0.116484619708477\n",
      "Epoch 735 => Avg Loss = 0.11636389916630933\n",
      "Epoch 736 => Avg Loss = 0.11624352124546693\n",
      "Epoch 737 => Avg Loss = 0.11612348485770561\n",
      "Epoch 738 => Avg Loss = 0.11600378868498525\n",
      "Epoch 739 => Avg Loss = 0.11588443135637147\n",
      "Epoch 740 => Avg Loss = 0.11576541149346989\n",
      "Epoch 741 => Avg Loss = 0.11564672772200385\n",
      "Epoch 742 => Avg Loss = 0.11552837867468092\n",
      "Epoch 743 => Avg Loss = 0.11541036299182285\n",
      "Epoch 744 => Avg Loss = 0.11529267932143118\n",
      "Epoch 745 => Avg Loss = 0.11517532631911413\n",
      "Epoch 746 => Avg Loss = 0.11505830264798803\n",
      "Epoch 747 => Avg Loss = 0.114941606978577\n",
      "Epoch 748 => Avg Loss = 0.11482523798871958\n",
      "Epoch 749 => Avg Loss = 0.1147091943634809\n",
      "Epoch 750 => Avg Loss = 0.11459347479507193\n",
      "Epoch 751 => Avg Loss = 0.1144780779827727\n",
      "Epoch 752 => Avg Loss = 0.11436300263286187\n",
      "Epoch 753 => Avg Loss = 0.11424824745854822\n",
      "Epoch 754 => Avg Loss = 0.11413381117990753\n",
      "Epoch 755 => Avg Loss = 0.11401969252382209\n",
      "Epoch 756 => Avg Loss = 0.11390589022392165\n",
      "Epoch 757 => Avg Loss = 0.11379240302052819\n",
      "Epoch 758 => Avg Loss = 0.11367922966060219\n",
      "Epoch 759 => Avg Loss = 0.11356636889769085\n",
      "Epoch 760 => Avg Loss = 0.11345381949187733\n",
      "Epoch 761 => Avg Loss = 0.11334158020973285\n",
      "Epoch 762 => Avg Loss = 0.1132296498242685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 763 => Avg Loss = 0.11311802711488926\n",
      "Epoch 764 => Avg Loss = 0.1130067108673482\n",
      "Epoch 765 => Avg Loss = 0.11289569987370304\n",
      "Epoch 766 => Avg Loss = 0.11278499293227198\n",
      "Epoch 767 => Avg Loss = 0.11267458884759113\n",
      "Epoch 768 => Avg Loss = 0.11256448643037256\n",
      "Epoch 769 => Avg Loss = 0.11245468449746328\n",
      "Epoch 770 => Avg Loss = 0.11234518187180374\n",
      "Epoch 771 => Avg Loss = 0.11223597738238876\n",
      "Epoch 772 => Avg Loss = 0.11212706986422687\n",
      "Epoch 773 => Avg Loss = 0.11201845815830191\n",
      "Epoch 774 => Avg Loss = 0.11191014111153387\n",
      "Epoch 775 => Avg Loss = 0.11180211757674091\n",
      "Epoch 776 => Avg Loss = 0.11169438641260161\n",
      "Epoch 777 => Avg Loss = 0.11158694648361724\n",
      "Epoch 778 => Avg Loss = 0.11147979666007472\n",
      "Epoch 779 => Avg Loss = 0.1113729358180103\n",
      "Epoch 780 => Avg Loss = 0.11126636283917295\n",
      "Epoch 781 => Avg Loss = 0.11116026252724429\n",
      "Epoch 782 => Avg Loss = 0.1110540096773179\n",
      "Epoch 783 => Avg Loss = 0.11094853432781895\n",
      "Epoch 784 => Avg Loss = 0.1108428625780005\n",
      "Epoch 785 => Avg Loss = 0.11073795765240427\n",
      "Epoch 786 => Avg Loss = 0.11063303519561654\n",
      "Epoch 787 => Avg Loss = 0.1105282515599528\n",
      "Epoch 788 => Avg Loss = 0.11042419601020087\n",
      "Epoch 789 => Avg Loss = 0.11031992974608766\n",
      "Epoch 790 => Avg Loss = 0.11021641987900539\n",
      "Epoch 791 => Avg Loss = 0.11011288867231796\n",
      "Epoch 792 => Avg Loss = 0.11000948963454459\n",
      "Epoch 793 => Avg Loss = 0.10990681207628926\n",
      "Epoch 794 => Avg Loss = 0.10980392078839016\n",
      "Epoch 795 => Avg Loss = 0.10970177862997854\n",
      "Epoch 796 => Avg Loss = 0.1095996121957251\n",
      "Epoch 797 => Avg Loss = 0.10949800890195753\n",
      "Epoch 798 => Avg Loss = 0.10939645061360445\n",
      "Epoch 799 => Avg Loss = 0.10929497053050367\n",
      "Epoch 800 => Avg Loss = 0.10919419234593614\n",
      "Epoch 801 => Avg Loss = 0.1090933789432879\n",
      "Epoch 802 => Avg Loss = 0.10899286606248472\n",
      "Epoch 803 => Avg Loss = 0.10889262639606555\n",
      "Epoch 804 => Avg Loss = 0.10879265186917496\n",
      "Epoch 805 => Avg Loss = 0.10869293958733516\n",
      "Epoch 806 => Avg Loss = 0.1085934880565914\n",
      "Epoch 807 => Avg Loss = 0.10849429616438401\n",
      "Epoch 808 => Avg Loss = 0.10839536290446745\n",
      "Epoch 809 => Avg Loss = 0.10829668730260744\n",
      "Epoch 810 => Avg Loss = 0.10819826839649707\n",
      "Epoch 811 => Avg Loss = 0.10810010523032423\n",
      "Epoch 812 => Avg Loss = 0.10800219685329876\n",
      "Epoch 813 => Avg Loss = 0.10790454231924913\n",
      "Epoch 814 => Avg Loss = 0.10780714068650811\n",
      "Epoch 815 => Avg Loss = 0.10770999101787314\n",
      "Epoch 816 => Avg Loss = 0.10761309238058878\n",
      "Epoch 817 => Avg Loss = 0.10751625901570509\n",
      "Epoch 818 => Avg Loss = 0.10742010575239734\n",
      "Epoch 819 => Avg Loss = 0.10732390929505335\n",
      "Epoch 820 => Avg Loss = 0.10722799330801254\n",
      "Epoch 821 => Avg Loss = 0.10713233277098105\n",
      "Epoch 822 => Avg Loss = 0.10703692021048634\n",
      "Epoch 823 => Avg Loss = 0.10694175293353056\n",
      "Epoch 824 => Avg Loss = 0.10684682955432243\n",
      "Epoch 825 => Avg Loss = 0.10675214904704709\n",
      "Epoch 826 => Avg Loss = 0.10665771048724802\n",
      "Epoch 827 => Avg Loss = 0.10656351298116169\n",
      "Epoch 828 => Avg Loss = 0.10646955564639257\n",
      "Epoch 829 => Avg Loss = 0.10637583760662085\n",
      "Epoch 830 => Avg Loss = 0.10628235799014678\n",
      "Epoch 831 => Avg Loss = 0.10618893168899947\n",
      "Epoch 832 => Avg Loss = 0.10609616833977102\n",
      "Epoch 833 => Avg Loss = 0.10600335623578258\n",
      "Epoch 834 => Avg Loss = 0.10591081009222061\n",
      "Epoch 835 => Avg Loss = 0.10581850658179652\n",
      "Epoch 836 => Avg Loss = 0.10572643868191595\n",
      "Epoch 837 => Avg Loss = 0.105634603850469\n",
      "Epoch 838 => Avg Loss = 0.10554300078107644\n",
      "Epoch 839 => Avg Loss = 0.10545162851050215\n",
      "Epoch 840 => Avg Loss = 0.10536048617283682\n",
      "Epoch 841 => Avg Loss = 0.10526957293174907\n",
      "Epoch 842 => Avg Loss = 0.10517888796179675\n",
      "Epoch 843 => Avg Loss = 0.1050882469103838\n",
      "Epoch 844 => Avg Loss = 0.10499825448789867\n",
      "Epoch 845 => Avg Loss = 0.10490820913228273\n",
      "Epoch 846 => Avg Loss = 0.10481841804016545\n",
      "Epoch 847 => Avg Loss = 0.10472885924963074\n",
      "Epoch 848 => Avg Loss = 0.10463952610494263\n",
      "Epoch 849 => Avg Loss = 0.10455041618691788\n",
      "Epoch 850 => Avg Loss = 0.10446152825326067\n",
      "Epoch 851 => Avg Loss = 0.10437286139077097\n",
      "Epoch 852 => Avg Loss = 0.10428441478016869\n",
      "Epoch 853 => Avg Loss = 0.1041960043773414\n",
      "Epoch 854 => Avg Loss = 0.10410823180537811\n",
      "Epoch 855 => Avg Loss = 0.10402040275611402\n",
      "Epoch 856 => Avg Loss = 0.1039328187103885\n",
      "Epoch 857 => Avg Loss = 0.10384545878696119\n",
      "Epoch 858 => Avg Loss = 0.10375831662264028\n",
      "Epoch 859 => Avg Loss = 0.10367138989641424\n",
      "Epoch 860 => Avg Loss = 0.10358467741678563\n",
      "Epoch 861 => Avg Loss = 0.10349817830991712\n",
      "Epoch 862 => Avg Loss = 0.1034117087149116\n",
      "Epoch 863 => Avg Loss = 0.10332586776642429\n",
      "Epoch 864 => Avg Loss = 0.10323996725565988\n",
      "Epoch 865 => Avg Loss = 0.10315430377292852\n",
      "Epoch 866 => Avg Loss = 0.10306885736716759\n",
      "Epoch 867 => Avg Loss = 0.10298362192856145\n",
      "Epoch 868 => Avg Loss = 0.10289859522121736\n",
      "Epoch 869 => Avg Loss = 0.10281377609739459\n",
      "Epoch 870 => Avg Loss = 0.10272916371693824\n",
      "Epoch 871 => Avg Loss = 0.10264457487089856\n",
      "Epoch 872 => Avg Loss = 0.10256060493519079\n",
      "Epoch 873 => Avg Loss = 0.10247657295929594\n",
      "Epoch 874 => Avg Loss = 0.10239277035892576\n",
      "Epoch 875 => Avg Loss = 0.10230917807486854\n",
      "Epoch 876 => Avg Loss = 0.10222579024162544\n",
      "Epoch 877 => Avg Loss = 0.10214260470543306\n",
      "Epoch 878 => Avg Loss = 0.1020596203604949\n",
      "Epoch 879 => Avg Loss = 0.10197665420154892\n",
      "Epoch 880 => Avg Loss = 0.10189429916611173\n",
      "Epoch 881 => Avg Loss = 0.10181187972186288\n",
      "Epoch 882 => Avg Loss = 0.10172968310889689\n",
      "Epoch 883 => Avg Loss = 0.10164769102936223\n",
      "Epoch 884 => Avg Loss = 0.1015658978275329\n",
      "Epoch 885 => Avg Loss = 0.10148430142030432\n",
      "Epoch 886 => Avg Loss = 0.10140290073780989\n",
      "Epoch 887 => Avg Loss = 0.101321513363005\n",
      "Epoch 888 => Avg Loss = 0.1012407289614011\n",
      "Epoch 889 => Avg Loss = 0.10115987819376243\n",
      "Epoch 890 => Avg Loss = 0.10107924394514037\n",
      "Epoch 891 => Avg Loss = 0.10099880865152767\n",
      "Epoch 892 => Avg Loss = 0.10091856686040417\n",
      "Epoch 893 => Avg Loss = 0.10083851655716453\n",
      "Epoch 894 => Avg Loss = 0.10075847511225887\n",
      "Epoch 895 => Avg Loss = 0.10067903063425383\n",
      "Epoch 896 => Avg Loss = 0.10059951772089279\n",
      "Epoch 897 => Avg Loss = 0.10052021598765921\n",
      "Epoch 898 => Avg Loss = 0.10044110848874871\n",
      "Epoch 899 => Avg Loss = 0.10036218994369767\n",
      "Epoch 900 => Avg Loss = 0.10028345839605297\n",
      "Epoch 901 => Avg Loss = 0.10020491283993081\n",
      "Epoch 902 => Avg Loss = 0.10012637199716899\n",
      "Epoch 903 => Avg Loss = 0.1000484194010757\n",
      "Epoch 904 => Avg Loss = 0.09997039708618205\n",
      "Epoch 905 => Avg Loss = 0.09989258004652868\n",
      "Epoch 906 => Avg Loss = 0.09981495202219381\n",
      "Epoch 907 => Avg Loss = 0.09973750792443557\n",
      "Epoch 908 => Avg Loss = 0.099660064953816\n",
      "Epoch 909 => Avg Loss = 0.09958320629368571\n",
      "Epoch 910 => Avg Loss = 0.09950627590318686\n",
      "Epoch 911 => Avg Loss = 0.09942954650532299\n",
      "Epoch 912 => Avg Loss = 0.09935300232878858\n",
      "Epoch 913 => Avg Loss = 0.09927663842211919\n",
      "Epoch 914 => Avg Loss = 0.0992004529402827\n",
      "Epoch 915 => Avg Loss = 0.09912426463667015\n",
      "Epoch 916 => Avg Loss = 0.0990486538688265\n",
      "Epoch 917 => Avg Loss = 0.09897297005374137\n",
      "Epoch 918 => Avg Loss = 0.09889748235629033\n",
      "Epoch 919 => Avg Loss = 0.09882217557339715\n",
      "Epoch 920 => Avg Loss = 0.09874704491384247\n",
      "Epoch 921 => Avg Loss = 0.09867208858680962\n",
      "Epoch 922 => Avg Loss = 0.09859712613055802\n",
      "Epoch 923 => Avg Loss = 0.09852273440514943\n",
      "Epoch 924 => Avg Loss = 0.09844826855830306\n",
      "Epoch 925 => Avg Loss = 0.09837399410940643\n",
      "Epoch 926 => Avg Loss = 0.09829989639925647\n",
      "Epoch 927 => Avg Loss = 0.09822597079067029\n",
      "Epoch 928 => Avg Loss = 0.09815203606797472\n",
      "Epoch 929 => Avg Loss = 0.0980786676419958\n",
      "Epoch 930 => Avg Loss = 0.09800522366018972\n",
      "Epoch 931 => Avg Loss = 0.09793196715096332\n",
      "Epoch 932 => Avg Loss = 0.09785888390142543\n",
      "Epoch 933 => Avg Loss = 0.09778596940135105\n",
      "Epoch 934 => Avg Loss = 0.0977130426557203\n",
      "Epoch 935 => Avg Loss = 0.09764067754823678\n",
      "Epoch 936 => Avg Loss = 0.0975682355828238\n",
      "Epoch 937 => Avg Loss = 0.0974959772572158\n",
      "Epoch 938 => Avg Loss = 0.09742388879837086\n",
      "Epoch 939 => Avg Loss = 0.09735196582165377\n",
      "Epoch 940 => Avg Loss = 0.09728002765538794\n",
      "Epoch 941 => Avg Loss = 0.09720864636768606\n",
      "Epoch 942 => Avg Loss = 0.09713718706628306\n",
      "Epoch 943 => Avg Loss = 0.09706590766973852\n",
      "Epoch 944 => Avg Loss = 0.09699472046749923\n",
      "Epoch 945 => Avg Loss = 0.0969238055334495\n",
      "Epoch 946 => Avg Loss = 0.09685294665922747\n",
      "Epoch 947 => Avg Loss = 0.09678260401366694\n",
      "Epoch 948 => Avg Loss = 0.09671217698716263\n",
      "Epoch 949 => Avg Loss = 0.09664192445115485\n",
      "Epoch 950 => Avg Loss = 0.0965722713694373\n",
      "Epoch 951 => Avg Loss = 0.09650235294768075\n",
      "Epoch 952 => Avg Loss = 0.09643258420842124\n",
      "Epoch 953 => Avg Loss = 0.09636296978303446\n",
      "Epoch 954 => Avg Loss = 0.09629351067279088\n",
      "Epoch 955 => Avg Loss = 0.09622420681389766\n",
      "Epoch 956 => Avg Loss = 0.09615505783194492\n",
      "Epoch 957 => Avg Loss = 0.09608606326358753\n",
      "Epoch 958 => Avg Loss = 0.09601722262160471\n",
      "Epoch 959 => Avg Loss = 0.09594853541392369\n",
      "Epoch 960 => Avg Loss = 0.09588000114911575\n",
      "Epoch 961 => Avg Loss = 0.09581161933792733\n",
      "Epoch 962 => Avg Loss = 0.09574338949365156\n",
      "Epoch 963 => Avg Loss = 0.09567531113217162\n",
      "Epoch 964 => Avg Loss = 0.09560738377191272\n",
      "Epoch 965 => Avg Loss = 0.09553960693377601\n",
      "Epoch 966 => Avg Loss = 0.09547198014107275\n",
      "Epoch 967 => Avg Loss = 0.09540450291946262\n",
      "Epoch 968 => Avg Loss = 0.09533717479689988\n",
      "Epoch 969 => Avg Loss = 0.09526999530358325\n",
      "Epoch 970 => Avg Loss = 0.0952029639719123\n",
      "Epoch 971 => Avg Loss = 0.09513608033644676\n",
      "Epoch 972 => Avg Loss = 0.09506934393387043\n",
      "Epoch 973 => Avg Loss = 0.09500275430295826\n",
      "Epoch 974 => Avg Loss = 0.09493631098454597\n",
      "Epoch 975 => Avg Loss = 0.094870013521502\n",
      "Epoch 976 => Avg Loss = 0.09480386145870248\n",
      "Epoch 977 => Avg Loss = 0.09473785434300683\n",
      "Epoch 978 => Avg Loss = 0.09467199172323594\n",
      "Epoch 979 => Avg Loss = 0.09460627315015209\n",
      "Epoch 980 => Avg Loss = 0.09454069817643831\n",
      "Epoch 981 => Avg Loss = 0.09447526635668181\n",
      "Epoch 982 => Avg Loss = 0.0944099772473555\n",
      "Epoch 983 => Avg Loss = 0.09434483040680233\n",
      "Epoch 984 => Avg Loss = 0.09427982539522031\n",
      "Epoch 985 => Avg Loss = 0.09421496177464654\n",
      "Epoch 986 => Avg Loss = 0.09415023910894439\n",
      "Epoch 987 => Avg Loss = 0.09408565696378884\n",
      "Epoch 988 => Avg Loss = 0.09402121490665502\n",
      "Epoch 989 => Avg Loss = 0.09395691250680381\n",
      "Epoch 990 => Avg Loss = 0.09389274933527154\n",
      "Epoch 991 => Avg Loss = 0.09382872496485696\n",
      "Epoch 992 => Avg Loss = 0.09376483897011081\n",
      "Epoch 993 => Avg Loss = 0.09370109092732415\n",
      "Epoch 994 => Avg Loss = 0.09363748041451797\n",
      "Epoch 995 => Avg Loss = 0.09357400701143254\n",
      "Epoch 996 => Avg Loss = 0.09351067029951726\n",
      "Epoch 997 => Avg Loss = 0.09344746986192064\n",
      "Epoch 998 => Avg Loss = 0.09338440528348055\n",
      "Epoch 999 => Avg Loss = 0.09332147615071423\n",
      "Epoch 1000 => Avg Loss = 0.09325868205180932\n",
      "Train loss = 0.09122396060330852\n",
      "Validation loss = 0.10478846539917572\n",
      "Validation accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "example_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88bdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

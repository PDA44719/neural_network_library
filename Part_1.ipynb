{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecd8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7183f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size, gain = 1.0):\n",
    "    \"\"\"\n",
    "    Xavier initialization of network weights.\n",
    "\n",
    "    Arguments:\n",
    "        - size {tuple} -- size of the network to initialise.\n",
    "        - gain {float} -- gain for the Xavier initialisation.\n",
    "\n",
    "    Returns:\n",
    "        {np.ndarray} -- values of the weights.\n",
    "    \"\"\"\n",
    "    low = -gain * np.sqrt(6.0 / np.sum(size))\n",
    "    high = gain * np.sqrt(6.0 / np.sum(size))\n",
    "    return np.random.uniform(low=low, high=high, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b73a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Abstract layer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_params(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37cbd21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELossLayer(Layer):\n",
    "    \"\"\"\n",
    "    MSELossLayer: Computes mean-squared error between y_pred and y_target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse(y_pred, y_target):\n",
    "        return np.mean((y_pred - y_target) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mse_grad(y_pred, y_target):\n",
    "        return 2 * (y_pred - y_target) / len(y_pred)\n",
    "\n",
    "    def forward(self, y_pred, y_target):\n",
    "        self._cache_current = y_pred, y_target\n",
    "        return self._mse(y_pred, y_target)\n",
    "\n",
    "    def backward(self):\n",
    "        return self._mse_grad(*self._cache_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c2fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossLayer(Layer):\n",
    "    \"\"\"\n",
    "    CrossEntropyLossLayer: Computes the softmax followed by the negative \n",
    "    log-likelihood loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._cache_current = None\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        numer = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        denom = numer.sum(axis=1, keepdims=True)\n",
    "        return numer / denom\n",
    "\n",
    "    def forward(self, inputs, y_target):\n",
    "        assert len(inputs) == len(y_target)\n",
    "        n_obs = len(y_target)\n",
    "        probs = self.softmax(inputs)\n",
    "        self._cache_current = y_target, probs\n",
    "\n",
    "        out = -1 / n_obs * np.sum(y_target * np.log(probs))\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        y_target, probs = self._cache_current\n",
    "        n_obs = len(y_target)\n",
    "        return -1 / n_obs * (y_target - probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4018f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer(Layer):\n",
    "    \"\"\"\n",
    "    SigmoidLayer: Applies sigmoid function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Constructor of the Sigmoid layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Sigmoid layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        output = 1/(1 + np.exp(-x))\n",
    "        self._cache_current = output\n",
    "        return output\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        output = self._cache_current\n",
    "        return grad_z * (1 - output) * output\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d1f6b",
   "metadata": {},
   "source": [
    "I believe this is how it works. Because, we are given $\\frac{\\partial Loss}{\\partial Z}$, and we want to obtain $\\frac{\\partial Loss}{\\partial X} = \\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial X}$. This is equal to multiplying the **grad_z** ($\\frac{\\partial Loss}{\\partial Z}$) by the derivative of the sigmoid function ($\\frac{\\partial Z}{\\partial X}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cabf663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88079708]\n",
      "[0.10499359]\n"
     ]
    }
   ],
   "source": [
    "s = SigmoidLayer()\n",
    "a = np.array([2])\n",
    "print(s.forward(a))\n",
    "print(s.backward(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccf58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d179423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1beb90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1050])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pda12_unix/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "t = torch.Tensor([2])\n",
    "l = torch.nn.Parameter(t)\n",
    "sigmoid(l).backward()\n",
    "print(l.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37e8402c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3303402067.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_477/3303402067.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sigmoid.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff3535f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_477/3692493142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "a = torch.transpose(t, 0, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568eccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a * value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a75d8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluLayer(Layer):\n",
    "    \"\"\"\n",
    "    ReluLayer: Applies Relu function elementwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor of the Relu layer.\n",
    "        \"\"\"\n",
    "        self._cache_current = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Performs forward pass through the Relu layer.\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        output =  x\n",
    "        output[output<=0] = 0\n",
    "        self._cache_current = output\n",
    "        return output\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        relu_derivative = self._cache_current\n",
    "        relu_derivative[relu_derivative > 0] = 1\n",
    "        return grad_z * relu_derivative\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92bb8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 5 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, -2, 5, 0])\n",
    "rl = ReluLayer()\n",
    "print(rl.forward(a))\n",
    "rl.backward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aa6ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3861,  0.3061,  0.1801,  0.3788,  0.0860],\n",
      "        [ 0.1389, -0.2401,  0.1812,  0.2646, -0.0514],\n",
      "        [-0.0290,  0.0824,  0.1846,  0.0314, -0.2791]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4321, -0.1411, -0.2414], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "l = torch.nn.Linear(5, 3)\n",
    "a = torch.Tensor([1, 0, 0, 0, 0])\n",
    "print(l.weight)\n",
    "l(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c91d89ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2])\n",
    "b = np.array([[2], [1]])\n",
    "np.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d102c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cecaadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([[1, 2], [3, 4]])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c09fe98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7192fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5abc2faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78625496, 0.96938018, 0.16065967, 0.95779647, 0.05078304],\n",
       "       [0.78625496, 0.96938018, 0.16065967, 0.95779647, 0.05078304],\n",
       "       [0.78625496, 0.96938018, 0.16065967, 0.95779647, 0.05078304],\n",
       "       [0.78625496, 0.96938018, 0.16065967, 0.95779647, 0.05078304],\n",
       "       [0.78625496, 0.96938018, 0.16065967, 0.95779647, 0.05078304]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_biases = np.random.rand(1, 5)\n",
    "np.concatenate([initial_biases] * 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64374c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.ones(shape=(1, 5)).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fbb67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \"\"\"\n",
    "    LinearLayer: Performs affine transformation of input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Constructor of the linear layer.\n",
    "\n",
    "        Arguments:\n",
    "            - n_in {int} -- Number (or dimension) of inputs.\n",
    "            - n_out {int} -- Number (or dimension) of outputs.\n",
    "        \"\"\"\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._W = None\n",
    "        self._b = None\n",
    "\n",
    "        self._cache_current = None\n",
    "        self._grad_W_current = None\n",
    "        self._grad_b_current = None\n",
    "        \n",
    "        self._W = np.random.rand(n_in, n_out)\n",
    "        self._b = np.random.rand(1, n_out)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the layer (i.e. returns Wx + b).\n",
    "\n",
    "        Logs information needed to compute gradient at a later stage in\n",
    "        `_cache_current`.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, n_in).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size, n_out)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._cache_current = x\n",
    "        B_matrix = np.concatenate([self._b] * n_in, 0)\n",
    "        return np.matmul(x, self._W)  + B_matrix\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Given `grad_z`, the gradient of some scalar (e.g. loss) with respect to\n",
    "        the output of this layer, performs back pass through the layer (i.e.\n",
    "        computes gradients of loss with respect to parameters of layer and\n",
    "        inputs of layer).\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size, n_out).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, n_in).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._grad_W_current = np.matmul(self._cache_current.transpose(), grad_z)\n",
    "        self._grad_b_current = np.matmul(np.ones(shape=(1, grad_z.shape[0])), grad_z)\n",
    "        return np.matmul(grad_z, self._W.transpose())\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        layer's parameters using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._b -= learning_rate * self._grad_b_current\n",
    "        self._W -= learning_rate * self._grad_W_current\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f88e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNetwork(object):\n",
    "    \"\"\"\n",
    "    MultiLayerNetwork: A network consisting of stacked linear layers and\n",
    "    activation functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, neurons, activations):\n",
    "        \"\"\"\n",
    "        Constructor of the multi layer network.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dim {int} -- Number of features in the input (excluding \n",
    "                the batch dimension).\n",
    "            - neurons {list} -- Number of neurons in each linear layer \n",
    "                represented as a list. The length of the list determines the \n",
    "                number of linear layers.\n",
    "            - activations {list} -- List of the activation functions to apply \n",
    "                to the output of each linear layer.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.neurons = neurons\n",
    "        self.activations = activations\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._layers = None\n",
    "        self._layers.append(LinearLayer(input_dim, neurons[0]))\n",
    "        \n",
    "        for i in range (1, len(neurons)):\n",
    "            self._layers.append(LinearLayer(neurons[i-1], neurons[i]))\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs forward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            x {np.ndarray} -- Input array of shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Output array of shape (batch_size,\n",
    "                #_neurons_in_final_layer)\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return np.zeros((1, self.neurons[-1])) # Replace with your own code\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def backward(self, grad_z):\n",
    "        \"\"\"\n",
    "        Performs backward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "            grad_z {np.ndarray} -- Gradient array of shape (batch_size,\n",
    "                #_neurons_in_final_layer).\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} -- Array containing gradient with respect to layer\n",
    "                input, of shape (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one step of gradient descent with given learning rate on the\n",
    "        parameters of all layers using currently stored gradients.\n",
    "\n",
    "        Arguments:\n",
    "            learning_rate {float} -- Learning rate of update step.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b229aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network(network, fpath):\n",
    "    \"\"\"\n",
    "    Utility function to pickle `network` at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"wb\") as f:\n",
    "        pickle.dump(network, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce11bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(fpath):\n",
    "    \"\"\"\n",
    "    Utility function to load network found at file path `fpath`.\n",
    "    \"\"\"\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        network = pickle.load(f)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e644d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trainer: Object that manages the training of a neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        network,\n",
    "        batch_size,\n",
    "        nb_epoch,\n",
    "        learning_rate,\n",
    "        loss_fun,\n",
    "        shuffle_flag,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor of the Trainer.\n",
    "\n",
    "        Arguments:\n",
    "            - network {MultiLayerNetwork} -- MultiLayerNetwork to be trained.\n",
    "            - batch_size {int} -- Training batch size.\n",
    "            - nb_epoch {int} -- Number of training epochs.\n",
    "            - learning_rate {float} -- SGD learning rate to be used in training.\n",
    "            - loss_fun {str} -- Loss function to be used. Possible values: mse,\n",
    "                cross_entropy.\n",
    "            - shuffle_flag {bool} -- If True, training data is shuffled before\n",
    "                training.\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fun = loss_fun\n",
    "        self.shuffle_flag = shuffle_flag\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._loss_layer = None\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle(input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Returns shuffled versions of the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_data_points, n_features) or (#_data_points,).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_data_points, #output_neurons).\n",
    "\n",
    "        Returns: \n",
    "            - {np.ndarray} -- shuffled inputs.\n",
    "            - {np.ndarray} -- shuffled_targets.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def train(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Main training loop. Performs the following steps `nb_epoch` times:\n",
    "            - Shuffles the input data (if `shuffle` is True)\n",
    "            - Splits the dataset into batches of size `batch_size`.\n",
    "            - For each batch:\n",
    "                - Performs forward pass through the network given the current\n",
    "                batch of inputs.\n",
    "                - Computes loss.\n",
    "                - Performs backward pass to compute gradients of loss with\n",
    "                respect to parameters of network.\n",
    "                - Performs one step of gradient descent on the network\n",
    "                parameters.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_training_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_training_data_points, #output_neurons).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def eval_loss(self, input_dataset, target_dataset):\n",
    "        \"\"\"\n",
    "        Function that evaluate the loss function for given data. Returns\n",
    "        scalar value.\n",
    "\n",
    "        Arguments:\n",
    "            - input_dataset {np.ndarray} -- Array of input features, of shape\n",
    "                (#_evaluation_data_points, n_features).\n",
    "            - target_dataset {np.ndarray} -- Array of corresponding targets, of\n",
    "                shape (#_evaluation_data_points, #output_neurons).\n",
    "        \n",
    "        Returns:\n",
    "            a scalar value -- the loss\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        pass\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51446815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    \"\"\"\n",
    "    Preprocessor: Object used to apply \"preprocessing\" operation to datasets.\n",
    "    The object can also be used to revert the changes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initializes the Preprocessor according to the provided dataset.\n",
    "        (Does not modify the dataset.)\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset used to determine the parameters for\n",
    "            the normalization.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        self._max_value = data.max()\n",
    "        self._min_value = data.min()\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def apply(self, data):\n",
    "        \"\"\"\n",
    "        Apply the pre-processing operations to the provided dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} normalized dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return (data - self._min_value) / (self._max_value - self._min_value)\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def revert(self, data):\n",
    "        \"\"\"\n",
    "        Revert the pre-processing operations to retrieve the original dataset.\n",
    "\n",
    "        Arguments:\n",
    "            data {np.ndarray} dataset for which to revert normalization.\n",
    "\n",
    "        Returns:\n",
    "            {np.ndarray} reverted dataset.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return data * (self._max_value - self._min_value) + self._min_value\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a6e3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.2]\n",
      " [1.  0.8]]\n",
      "[[-2.  0.]\n",
      " [ 8.  6.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[-2,  0], [8, 6]])\n",
    "prep = Preprocessor(a)\n",
    "a_preprocessed = prep.apply(a)\n",
    "print(a_preprocessed)\n",
    "a_reverted = prep.revert(a_preprocessed)\n",
    "print(a_reverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35efc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_main():\n",
    "    input_dim = 4\n",
    "    neurons = [16, 3]\n",
    "    activations = [\"relu\", \"identity\"]\n",
    "    net = MultiLayerNetwork(input_dim, neurons, activations)\n",
    "\n",
    "    dat = np.loadtxt(\"iris.dat\")\n",
    "    np.random.shuffle(dat)\n",
    "\n",
    "    x = dat[:, :4]\n",
    "    y = dat[:, 4:]\n",
    "\n",
    "    split_idx = int(0.8 * len(x))\n",
    "\n",
    "    x_train = x[:split_idx]\n",
    "    y_train = y[:split_idx]\n",
    "    x_val = x[split_idx:]\n",
    "    y_val = y[split_idx:]\n",
    "\n",
    "    prep_input = Preprocessor(x_train)\n",
    "\n",
    "    x_train_pre = prep_input.apply(x_train)\n",
    "    x_val_pre = prep_input.apply(x_val)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        network=net,\n",
    "        batch_size=8,\n",
    "        nb_epoch=1000,\n",
    "        learning_rate=0.01,\n",
    "        loss_fun=\"cross_entropy\",\n",
    "        shuffle_flag=True,\n",
    "    )\n",
    "\n",
    "    trainer.train(x_train_pre, y_train)\n",
    "    print(f\"Train loss = {trainer.eval_loss(x_train_pre, y_train)}\")\n",
    "    print(f\"Validation loss = {trainer.eval_loss(x_val_pre, y_val)}\")\n",
    "\n",
    "    preds = net(x_val_pre).argmax(axis=1).squeeze()\n",
    "    targets = y_val.argmax(axis=1).squeeze()\n",
    "    accuracy = (preds == targets).mean()\n",
    "    print(f\"Validation accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce01a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
